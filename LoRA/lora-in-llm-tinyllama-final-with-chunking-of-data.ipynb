{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:07:20.424705Z","iopub.execute_input":"2025-06-16T10:07:20.425102Z","iopub.status.idle":"2025-06-16T10:07:26.506606Z","shell.execute_reply.started":"2025-06-16T10:07:20.425070Z","shell.execute_reply":"2025-06-16T10:07:26.505118Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# =====================\n# LoRA Layer Definition\n# =====================\nclass LoRALinear(nn.Module):\n    def __init__(self, original, r=8, alpha=16):\n        super().__init__()\n        self.original = original\n        self.r = r\n        self.alpha = alpha\n        self.scaling = alpha / r\n\n        self.lora_A = nn.Parameter(torch.randn(r, original.in_features) * 0.01)\n        self.lora_B = nn.Parameter(torch.randn(original.out_features, r) * 0.01)\n\n    def forward(self, x):\n        return self.original(x) + ((x @ self.lora_A.T) @ self.lora_B.T) * self.scaling\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:07:35.942652Z","iopub.execute_input":"2025-06-16T10:07:35.943043Z","iopub.status.idle":"2025-06-16T10:07:52.386012Z","shell.execute_reply.started":"2025-06-16T10:07:35.943005Z","shell.execute_reply":"2025-06-16T10:07:52.384759Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# LoRA Injection into Attention Layers\n\ndef apply_lora(model, target_modules=(\"q_proj\", \"v_proj\"), r=8, alpha=16):\n    for name, module in model.named_modules():\n        if name.endswith(target_modules):\n            parts = name.split(\".\")\n            parent = model\n            for p in parts[:-1]:\n                if p.isdigit():\n                    parent = parent[int(p)]\n                else:\n                    parent = getattr(parent, p)\n            layer_name = parts[-1]\n            original = getattr(parent, layer_name)\n            if isinstance(original, nn.Linear):\n                setattr(parent, layer_name, LoRALinear(original, r=r, alpha=alpha))   #REPLACES THE ORIGINAL LAYER WITH LoRA LAYER\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:08:00.826770Z","iopub.execute_input":"2025-06-16T10:08:00.830850Z","iopub.status.idle":"2025-06-16T10:08:00.849048Z","shell.execute_reply.started":"2025-06-16T10:08:00.830413Z","shell.execute_reply":"2025-06-16T10:08:00.847319Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ==========================\n# Count Parameters Utilities\n# ==========================\ndef print_trainable_params(model, label=\"\"):\n    total = sum(p.numel() for p in model.parameters())\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)  #includes parameters that are being updated during training\n    percent = 100 * trainable / total   #WHAT PERCENT OF TOTAL PARAMETERS ARE TRAINABLE\n    print(f\"{label} Model - Total: {total:,}, Trainable: {trainable:,} ({percent:.4f}%)\")\n    return total, trainable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:08:03.928362Z","iopub.execute_input":"2025-06-16T10:08:03.928777Z","iopub.status.idle":"2025-06-16T10:08:03.936598Z","shell.execute_reply.started":"2025-06-16T10:08:03.928746Z","shell.execute_reply":"2025-06-16T10:08:03.934750Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ======================\n# Dataset + Tokenization\n# ======================\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\ndataset = load_dataset(\"tiny_shakespeare\")[\"train\"]\nprint(f\"Dataset size: {len(dataset)}\")\ndataset = dataset.select(range(min(1000, len(dataset))))\n\n\ndef tokenize(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ndataset = dataset.map(tokenize)\n\ndef collate_fn(batch):\n    input_ids = torch.tensor([b[\"input_ids\"] for b in batch])\n    return {\"input_ids\": input_ids, \"labels\": input_ids}\n\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:08:08.873813Z","iopub.execute_input":"2025-06-16T10:08:08.874777Z","iopub.status.idle":"2025-06-16T10:08:10.753665Z","shell.execute_reply.started":"2025-06-16T10:08:08.874745Z","shell.execute_reply":"2025-06-16T10:08:10.752472Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ================\n# Evaluation Logic\n# ================\ndef evaluate(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    loss_fn = nn.CrossEntropyLoss()\n    with torch.no_grad():\n        for batch in dataloader:\n            inputs = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**inputs)\n            logits = outputs.logits\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = inputs[\"labels\"][..., 1:].contiguous()\n\n            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n            total_loss += loss.item()\n\n            preds = shift_logits.argmax(dim=-1)\n            mask = shift_labels != -100\n            correct += (preds == shift_labels).masked_select(mask).sum().item()\n            total += mask.sum().item()\n\n    return total_loss / len(dataloader), correct / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:08:14.784950Z","iopub.execute_input":"2025-06-16T10:08:14.785321Z","iopub.status.idle":"2025-06-16T10:08:14.793247Z","shell.execute_reply.started":"2025-06-16T10:08:14.785291Z","shell.execute_reply":"2025-06-16T10:08:14.792155Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ====================\n# Load Model Function\n# ====================\ndef get_model():\n    model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n    return model\n\n# ================\n# Main Training Run\n# ================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ---- Evaluate base model ----\nprint(\"\\n🔍 Evaluating Base Model\")\nbase_model1 = get_model().to(device)\nprint_trainable_params(base_model1, label=\"Base\")\nbase_loss, base_acc = evaluate(base_model1, dataloader, device)\nprint(f\"✅ Base Model -> Loss: {base_loss:.4f}, Accuracy: {base_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:08:25.793547Z","iopub.execute_input":"2025-06-16T10:08:25.793896Z","iopub.status.idle":"2025-06-16T10:08:54.563469Z","shell.execute_reply.started":"2025-06-16T10:08:25.793854Z","shell.execute_reply":"2025-06-16T10:08:54.562295Z"}},"outputs":[{"name":"stdout","text":"\n🔍 Evaluating Base Model\n","output_type":"stream"},{"name":"stderr","text":"2025-06-16 10:08:28.278008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750068508.536516     160 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750068508.615366     160 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Base Model - Total: 1,100,048,384, Trainable: 1,100,048,384 (100.0000%)\n✅ Base Model -> Loss: 2.4788, Accuracy: 0.5276\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ---- Apply LoRA ----\nprint(\"\\n🧩 Applying LoRA and Fine-Tuning\")\nlora_model = get_model().to(device)\napply_lora(lora_model, r=8, alpha=16)\n\n# Freeze base model\nfor param in lora_model.parameters():\n    param.requires_grad = False\nfor module in lora_model.modules():\n    if isinstance(module, LoRALinear):\n        module.lora_A.requires_grad = True\n        module.lora_B.requires_grad = True\n\nprint_trainable_params(lora_model, label=\"LoRA\")\n\n# ---- Train LoRA ----\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr=5e-4)\n\nlora_model.train()\nfor epoch in range(3):\n    print(f\"\\n🚀 Epoch {epoch+1}\")\n    for batch in tqdm(dataloader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = lora_model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:10:10.081084Z","iopub.execute_input":"2025-06-16T10:10:10.082947Z","iopub.status.idle":"2025-06-16T10:10:29.885729Z","shell.execute_reply.started":"2025-06-16T10:10:10.082892Z","shell.execute_reply":"2025-06-16T10:10:29.885071Z"}},"outputs":[{"name":"stdout","text":"\n🧩 Applying LoRA and Fine-Tuning\nLoRA Model - Total: 1,101,174,784, Trainable: 1,126,400 (0.1023%)\n\n🚀 Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n🚀 Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:05<00:00,  5.32s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n🚀 Epoch 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:05<00:00,  5.13s/it]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ---- Evaluate LoRA Model ----\nprint(\"\\n🔍 Evaluating LoRA Model\")\nlora_loss, lora_acc = evaluate(lora_model, dataloader, device)\nprint(f\"✅ LoRA Model -> Loss: {lora_loss:.4f}, Accuracy: {lora_acc:.4f}\")\n\n# ---- Comparison ----\nprint(\"\\n📊 === Final Comparison ===\")\nprint(f\"Base Model: Loss = {base_loss:.4f}, Accuracy = {base_acc:.4f}\")\nprint(f\"LoRA Model: Loss = {lora_loss:.4f}, Accuracy = {lora_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:10:35.693069Z","iopub.execute_input":"2025-06-16T10:10:35.693764Z","iopub.status.idle":"2025-06-16T10:10:38.365700Z","shell.execute_reply.started":"2025-06-16T10:10:35.693736Z","shell.execute_reply":"2025-06-16T10:10:38.364554Z"}},"outputs":[{"name":"stdout","text":"\n🔍 Evaluating LoRA Model\n✅ LoRA Model -> Loss: 1.9137, Accuracy: 0.6614\n\n📊 === Final Comparison ===\nBase Model: Loss = 2.4788, Accuracy = 0.5276\nLoRA Model: Loss = 1.9137, Accuracy = 0.6614\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**PEFT**","metadata":{}},{"cell_type":"code","source":"pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:10:45.320847Z","iopub.execute_input":"2025-06-16T10:10:45.321235Z","iopub.status.idle":"2025-06-16T10:10:49.802520Z","shell.execute_reply.started":"2025-06-16T10:10:45.321209Z","shell.execute_reply":"2025-06-16T10:10:49.801292Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.31.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2025.3.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.4.26)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import get_peft_model, LoraConfig, TaskType\nimport torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:10:52.084495Z","iopub.execute_input":"2025-06-16T10:10:52.084922Z","iopub.status.idle":"2025-06-16T10:10:52.274468Z","shell.execute_reply.started":"2025-06-16T10:10:52.084886Z","shell.execute_reply":"2025-06-16T10:10:52.273387Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:10:54.794463Z","iopub.execute_input":"2025-06-16T10:10:54.795505Z","iopub.status.idle":"2025-06-16T10:10:57.692601Z","shell.execute_reply.started":"2025-06-16T10:10:54.795471Z","shell.execute_reply":"2025-06-16T10:10:57.691668Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Load Base Model\n# ====================\nimport copy\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbase_model2 = AutoModelForCausalLM.from_pretrained(model_name).to(device)\nbase_model_copy2 = copy.deepcopy(base_model2)\n\nprint(\"\\n🔍 Evaluating Base Model\")\nprint_trainable_params(base_model_copy2, label=\"Base\")\nbase_loss2, base_acc2 = evaluate(base_model_copy2, dataloader, device)\nprint(f\"✅ Base Model -> Loss: {base_loss2:.4f}, Accuracy: {base_acc2:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:11:48.530038Z","iopub.execute_input":"2025-06-16T10:11:48.531199Z","iopub.status.idle":"2025-06-16T10:11:56.223597Z","shell.execute_reply.started":"2025-06-16T10:11:48.531163Z","shell.execute_reply":"2025-06-16T10:11:56.222427Z"}},"outputs":[{"name":"stdout","text":"\n🔍 Evaluating Base Model\nBase Model - Total: 1,100,048,384, Trainable: 1,100,048,384 (100.0000%)\n✅ Base Model -> Loss: 2.4788, Accuracy: 0.5276\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],  # depends on model architecture\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM  # since this is a language model\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:11:59.925423Z","iopub.execute_input":"2025-06-16T10:11:59.926442Z","iopub.status.idle":"2025-06-16T10:11:59.931572Z","shell.execute_reply.started":"2025-06-16T10:11:59.926364Z","shell.execute_reply":"2025-06-16T10:11:59.930458Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"LoraConfig?","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:14:51.631761Z","iopub.execute_input":"2025-06-16T10:14:51.632879Z","iopub.status.idle":"2025-06-16T10:14:51.687148Z","shell.execute_reply.started":"2025-06-16T10:14:51.632822Z","shell.execute_reply":"2025-06-16T10:14:51.685961Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mLoraConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtask_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTaskType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpeft_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPeftType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mauto_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbase_model_name_or_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0minference_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtarget_modules\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[list[str], str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mexclude_modules\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[list[str], str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlora_alpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlora_dropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfan_in_fan_out\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Literal['none', 'all', 'lora_only']\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0muse_rslora\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmodules_to_save\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[list[str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0minit_lora_weights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"bool | Literal['gaussian', 'eva', 'olora', 'pissa', 'pissa_niter_[number of iters]', 'loftq']\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlayers_to_transform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[list[int], int]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlayers_pattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[list[str], str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrank_pattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0malpha_pattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmegatron_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmegatron_core\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'megatron.core'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mloftq_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Union[LoftQConfig, dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0meva_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[EvaConfig]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0muse_dora\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlayer_replication\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[list[tuple[int, int]]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mruntime_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'LoraRuntimeConfig'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlora_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nThis is the configuration class to store the configuration of a [`LoraModel`].\n\nArgs:\n    r (`int`):\n        Lora attention dimension (the \"rank\").\n    target_modules (`Optional[Union[List[str], str]]`):\n        The names of the modules to apply the adapter to. If this is specified, only the modules with the specified\n        names will be replaced. When passing a string, a regex match will be performed. When passing a list of\n        strings, either an exact match will be performed or it is checked if the name of the module ends with any\n        of the passed strings. If this is specified as 'all-linear', then all linear/Conv1D modules are chosen,\n        excluding the output layer. If this is not specified, modules will be chosen according to the model\n        architecture. If the architecture is not known, an error will be raised -- in this case, you should specify\n        the target modules manually.\n    exclude_modules (`Optional[Union[List[str], str]]`):\n        The names of the modules to not apply the adapter. When passing a string, a regex match will be performed.\n        When passing a list of strings, either an exact match will be performed or it is checked if the name of the\n        module ends with any of the passed strings.\n    lora_alpha (`int`):\n        The alpha parameter for Lora scaling.\n    lora_dropout (`float`):\n        The dropout probability for Lora layers.\n    fan_in_fan_out (`bool`):\n        Set this to True if the layer to replace stores weight like (fan_in, fan_out). For example, gpt-2 uses\n        `Conv1D` which stores weights like (fan_in, fan_out) and hence this should be set to `True`.\n    bias (`str`):\n        Bias type for LoRA. Can be 'none', 'all' or 'lora_only'. If 'all' or 'lora_only', the corresponding biases\n        will be updated during training. Be aware that this means that, even when disabling the adapters, the model\n        will not produce the same output as the base model would have without adaptation.\n    use_rslora (`bool`):\n        When set to True, uses <a href='https://doi.org/10.48550/arXiv.2312.03732'>Rank-Stabilized LoRA</a> which\n        sets the adapter scaling factor to `lora_alpha/math.sqrt(r)`, since it was proven to work better.\n        Otherwise, it will use the original default value of `lora_alpha/r`.\n    modules_to_save (`List[str]`):\n        List of modules apart from adapter layers to be set as trainable and saved in the final checkpoint.\n    init_lora_weights (`bool` | `Literal[\"gaussian\", \"eva\", \"olora\", \"pissa\", \"pissa_niter_[number of iters]\", \"loftq\"]`):\n        How to initialize the weights of the adapter layers. Passing True (default) results in the default\n        initialization from the reference implementation from Microsoft. Passing 'gaussian' results in Gaussian\n        initialization scaled by the LoRA rank for linear and layers. Setting the initialization to False leads to\n        completely random initialization and is discouraged. Pass `'loftq'` to use LoftQ initialization. Passing\n        `'eva'` results in a data-driven initialization of <ahref='https://arxiv.org/abs/2410.07170' >Explained\n        Variance Adaptation</a>. EVA initalizes LoRA based on the SVD of layer input activations and achieves SOTA\n        performance due to its ability to adapt to the finetuning data. Pass `'olora'` to use OLoRA initialization.\n        Passing `'pissa'` results in the initialization of <ahref='https://arxiv.org/abs/2404.02948' >Principal\n        Singular values and Singular vectors Adaptation (PiSSA)</a>, which converges more rapidly than LoRA and\n        ultimately achieves superior performance. Moreover, PiSSA reduces the quantization error compared to QLoRA,\n        leading to further enhancements. Passing `'pissa_niter_[number of iters]'` initiates Fast-SVD-based PiSSA\n        initialization, where `[number of iters]` indicates the number of subspace iterations to perform FSVD, and\n        must be a nonnegative integer. When `[number of iters]` is set to 16, it can complete the initialization of\n        a 7B model within seconds, and the training effect is approximately equivalent to using SVD.\n    layers_to_transform (`Union[List[int], int]`):\n        The layer indices to transform. If a list of ints is passed, it will apply the adapter to the layer indices\n        that are specified in this list. If a single integer is passed, it will apply the transformations on the\n        layer at this index.\n    layers_pattern (`Optional[Union[List[str], str]]`):\n        The layer pattern name, used only if `layers_to_transform` is different from `None`. This should target the\n        `nn.ModuleList` of the model, which is often called `'layers'` or `'h'`.\n    rank_pattern (`dict`):\n        The mapping from layer names or regexp expression to ranks which are different from the default rank\n        specified by `r`.\n    alpha_pattern (`dict`):\n        The mapping from layer names or regexp expression to alphas which are different from the default alpha\n        specified by `lora_alpha`.\n    megatron_config (`Optional[dict]`):\n        The TransformerConfig arguments for Megatron. It is used to create LoRA's parallel linear layer. You can\n        get it like this, `core_transformer_config_from_args(get_args())`, these two functions being from Megatron.\n        The arguments will be used to initialize the TransformerConfig of Megatron. You need to specify this\n        parameter when you want to apply LoRA to the ColumnParallelLinear and RowParallelLinear layers of megatron.\n    megatron_core (`Optional[str]`):\n        The core module from Megatron to use, defaults to `\"megatron.core\"`.\n    loftq_config (`Optional[LoftQConfig]`):\n        The configuration of LoftQ. If this is not None, then LoftQ will be used to quantize the backbone weights\n        and initialize Lora layers. Also pass `init_lora_weights='loftq'`. Note that you should not pass a\n        quantized model in this case, as LoftQ will quantize the model itself.\n    eva_config (`Optional[EvaConfig]`):\n        The configuration of EVA. At a minimum the dataset argument needs to be set (use the same dataset as for\n        finetuning).\n    use_dora (`bool`):\n        Enable 'Weight-Decomposed Low-Rank Adaptation' (DoRA). This technique decomposes the updates of the weights\n        into two parts, magnitude and direction. Direction is handled by normal LoRA, whereas the magnitude is\n        handled by a separate learnable parameter. This can improve the performance of LoRA especially at low\n        ranks. Right now, DoRA only supports linear and Conv2D layers. DoRA introduces a bigger overhead than pure\n        LoRA, so it is recommended to merge weights for inference. For more information, see\n        https://arxiv.org/abs/2402.09353.\n    layer_replication (`List[Tuple[int, int]]`):\n        Build a new stack of layers by stacking the original model layers according to the ranges specified. This\n        allows expanding (or shrinking) the model without duplicating the base model weights. The new layers will\n        all have separate LoRA adapters attached to them.\n    runtime_config (`LoraRuntimeConfig`):\n        Runtime configurations (which are not saved or restored).\n    lora_bias (`bool`):\n        Defaults to `False`. Whether to enable the bias term for the LoRA B parameter. Typically, this should be\n        disabled. The main use case for this is when the LoRA weights were extracted from fully fine-tuned\n        parameters so the bias of those parameters can be taken into account.\n\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.11/dist-packages/peft/tuners/lora/config.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     AdaLoraConfig\n"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"WRAPPING THE MODEL WITH PEFT","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training  # if using 8-bit or 4-bit, optional\n# model = prepare_model_for_kbit_training(model)  # Only if using quantized model\n\npeft_model = get_peft_model(base_model2, lora_config)\n\n\npeft_model = peft_model.to(device)\nprint(\"\\n🧩 LoRA-Injected Model\")\nprint_trainable_params(peft_model, label=\"LoRA\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:12:11.473824Z","iopub.execute_input":"2025-06-16T10:12:11.474194Z","iopub.status.idle":"2025-06-16T10:12:11.585453Z","shell.execute_reply.started":"2025-06-16T10:12:11.474171Z","shell.execute_reply":"2025-06-16T10:12:11.584248Z"}},"outputs":[{"name":"stdout","text":"\n🧩 LoRA-Injected Model\nLoRA Model - Total: 1,101,174,784, Trainable: 1,126,400 (0.1023%)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(1101174784, 1126400)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Training Loop\n# ====================\nfrom torch.optim import AdamW\npeft_model.train()\noptimizer = AdamW(peft_model.parameters(), lr=1e-4)\n\nfor epoch in range(3):\n    total_loss = 0\n    for batch in dataloader:\n        input_ids = batch[\"input_ids\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = peft_model(input_ids=input_ids, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(dataloader)\n    print(f\"📚 Epoch {epoch+1} -> Training Loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:12:41.218024Z","iopub.execute_input":"2025-06-16T10:12:41.218401Z","iopub.status.idle":"2025-06-16T10:12:57.846006Z","shell.execute_reply.started":"2025-06-16T10:12:41.218375Z","shell.execute_reply":"2025-06-16T10:12:57.844885Z"}},"outputs":[{"name":"stdout","text":"📚 Epoch 1 -> Training Loss: 2.4788\n📚 Epoch 2 -> Training Loss: 2.4510\n📚 Epoch 3 -> Training Loss: 2.4219\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Evaluation After Training\n# ====================\nprint(\"\\nEvaluating Fine-Tuned LoRA Model\")\nfinal_loss, final_acc = evaluate(peft_model, dataloader, device)\nprint(f\"Base Model: Loss = {base_loss2:.4f}, Accuracy = {base_acc2:.4f}\")\nprint(f\" LoRA Model -> Loss: {final_loss:.4f}, Accuracy: {final_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:13:16.914294Z","iopub.execute_input":"2025-06-16T10:13:16.914634Z","iopub.status.idle":"2025-06-16T10:13:19.532171Z","shell.execute_reply.started":"2025-06-16T10:13:16.914613Z","shell.execute_reply":"2025-06-16T10:13:19.531088Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating Fine-Tuned LoRA Model\nBase Model: Loss = 2.4788, Accuracy = 0.5276\n LoRA Model -> Loss: 2.3910, Accuracy: 0.5591\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print_trainable_params(base_model1, label=\"Base Model\")\nprint_trainable_params(lora_model, label=\"Manual LoRA Model\")\nprint_trainable_params(peft_model, label=\"PEFT LoRA Model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T10:13:37.868764Z","iopub.execute_input":"2025-06-16T10:13:37.869566Z","iopub.status.idle":"2025-06-16T10:13:37.885920Z","shell.execute_reply.started":"2025-06-16T10:13:37.869531Z","shell.execute_reply":"2025-06-16T10:13:37.884705Z"}},"outputs":[{"name":"stdout","text":"Base Model Model - Total: 1,100,048,384, Trainable: 1,100,048,384 (100.0000%)\nManual LoRA Model Model - Total: 1,101,174,784, Trainable: 1,126,400 (0.1023%)\nPEFT LoRA Model Model - Total: 1,101,174,784, Trainable: 1,126,400 (0.1023%)\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(1101174784, 1126400)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}