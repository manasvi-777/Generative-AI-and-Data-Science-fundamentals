{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install auto-gptq transformers accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T12:37:47.611365Z","iopub.execute_input":"2025-06-17T12:37:47.611650Z","iopub.status.idle":"2025-06-17T12:37:51.322301Z","shell.execute_reply.started":"2025-06-17T12:37:47.611630Z","shell.execute_reply":"2025-06-17T12:37:51.321379Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: auto-gptq in /usr/local/lib/python3.11/dist-packages (0.7.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (3.6.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.26.4)\nRequirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.0.1)\nRequirement already satisfied: gekko in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.3.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (2.6.0+cu124)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.5.3)\nRequirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.14.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq) (1.3.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (0.70.16)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge->auto-gptq) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (3.11.18)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->auto-gptq) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->auto-gptq) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->auto-gptq) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->auto-gptq) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (1.20.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->auto-gptq) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torch\nimport time\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\n# --- Settings ---\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nquantized_model_dir = \"./tinyllama_1_1B_chat_gptq_4bit\"\nprompt = \"The capital of France is\"\n\n# --- Load tokenizer ---\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T12:38:16.667183Z","iopub.execute_input":"2025-06-17T12:38:16.667476Z","iopub.status.idle":"2025-06-17T12:38:16.820027Z","shell.execute_reply.started":"2025-06-17T12:38:16.667451Z","shell.execute_reply":"2025-06-17T12:38:16.819407Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# === STEP 1: QUANTIZE FULL-PRECISION MODEL ===\nprint(\"\\n[Step 1] Quantizing full-precision model...\")\n\nquant_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    desc_act=False,\n    damp_percent=0.01,\n    true_sequential=True,\n    model_name_or_path=model_id\n)\n\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    model_id,\n    quantize_config=quant_config,\n    torch_dtype=torch.float16\n)\n\n# Dummy calibration data\ntexts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"To be or not to be, that is the question.\",\n    \"A journey of a thousand miles begins with a single step.\",\n    \"All that glitters is not gold.\"\n] * 10\n\ncalib_dataset = [\n    tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n    for text in texts\n]\nmodel.quantize(calib_dataset)\nmodel.save_pretrained(quantized_model_dir)\ntokenizer.save_pretrained(quantized_model_dir)\nprint(\"✅ Quantization complete and saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T12:41:46.403750Z","iopub.execute_input":"2025-06-17T12:41:46.404414Z","iopub.status.idle":"2025-06-17T12:45:31.614589Z","shell.execute_reply.started":"2025-06-17T12:41:46.404388Z","shell.execute_reply":"2025-06-17T12:45:31.613746Z"}},"outputs":[{"name":"stdout","text":"\n[Step 1] Quantizing full-precision model...\n","output_type":"stream"},{"name":"stderr","text":"INFO - Start quantizing layer 1/22\nINFO - Quantizing self_attn.k_proj in layer 1/22...\nINFO - Quantizing self_attn.v_proj in layer 1/22...\nINFO - Quantizing self_attn.q_proj in layer 1/22...\nINFO - Quantizing self_attn.o_proj in layer 1/22...\nINFO - Quantizing mlp.up_proj in layer 1/22...\nINFO - Quantizing mlp.gate_proj in layer 1/22...\nINFO - Quantizing mlp.down_proj in layer 1/22...\nINFO - Start quantizing layer 2/22\nINFO - Quantizing self_attn.k_proj in layer 2/22...\nINFO - Quantizing self_attn.v_proj in layer 2/22...\nINFO - Quantizing self_attn.q_proj in layer 2/22...\nINFO - Quantizing self_attn.o_proj in layer 2/22...\nINFO - Quantizing mlp.up_proj in layer 2/22...\nINFO - Quantizing mlp.gate_proj in layer 2/22...\nINFO - Quantizing mlp.down_proj in layer 2/22...\nINFO - Start quantizing layer 3/22\nINFO - Quantizing self_attn.k_proj in layer 3/22...\nINFO - Quantizing self_attn.v_proj in layer 3/22...\nINFO - Quantizing self_attn.q_proj in layer 3/22...\nINFO - Quantizing self_attn.o_proj in layer 3/22...\nINFO - Quantizing mlp.up_proj in layer 3/22...\nINFO - Quantizing mlp.gate_proj in layer 3/22...\nINFO - Quantizing mlp.down_proj in layer 3/22...\nINFO - Start quantizing layer 4/22\nINFO - Quantizing self_attn.k_proj in layer 4/22...\nINFO - Quantizing self_attn.v_proj in layer 4/22...\nINFO - Quantizing self_attn.q_proj in layer 4/22...\nINFO - Quantizing self_attn.o_proj in layer 4/22...\nINFO - Quantizing mlp.up_proj in layer 4/22...\nINFO - Quantizing mlp.gate_proj in layer 4/22...\nINFO - Quantizing mlp.down_proj in layer 4/22...\nINFO - Start quantizing layer 5/22\nINFO - Quantizing self_attn.k_proj in layer 5/22...\nINFO - Quantizing self_attn.v_proj in layer 5/22...\nINFO - Quantizing self_attn.q_proj in layer 5/22...\nINFO - Quantizing self_attn.o_proj in layer 5/22...\nINFO - Quantizing mlp.up_proj in layer 5/22...\nINFO - Quantizing mlp.gate_proj in layer 5/22...\nINFO - Quantizing mlp.down_proj in layer 5/22...\nINFO - Start quantizing layer 6/22\nINFO - Quantizing self_attn.k_proj in layer 6/22...\nINFO - Quantizing self_attn.v_proj in layer 6/22...\nINFO - Quantizing self_attn.q_proj in layer 6/22...\nINFO - Quantizing self_attn.o_proj in layer 6/22...\nINFO - Quantizing mlp.up_proj in layer 6/22...\nINFO - Quantizing mlp.gate_proj in layer 6/22...\nINFO - Quantizing mlp.down_proj in layer 6/22...\nINFO - Start quantizing layer 7/22\nINFO - Quantizing self_attn.k_proj in layer 7/22...\nINFO - Quantizing self_attn.v_proj in layer 7/22...\nINFO - Quantizing self_attn.q_proj in layer 7/22...\nINFO - Quantizing self_attn.o_proj in layer 7/22...\nINFO - Quantizing mlp.up_proj in layer 7/22...\nINFO - Quantizing mlp.gate_proj in layer 7/22...\nINFO - Quantizing mlp.down_proj in layer 7/22...\nINFO - Start quantizing layer 8/22\nINFO - Quantizing self_attn.k_proj in layer 8/22...\nINFO - Quantizing self_attn.v_proj in layer 8/22...\nINFO - Quantizing self_attn.q_proj in layer 8/22...\nINFO - Quantizing self_attn.o_proj in layer 8/22...\nINFO - Quantizing mlp.up_proj in layer 8/22...\nINFO - Quantizing mlp.gate_proj in layer 8/22...\nINFO - Quantizing mlp.down_proj in layer 8/22...\nINFO - Start quantizing layer 9/22\nINFO - Quantizing self_attn.k_proj in layer 9/22...\nINFO - Quantizing self_attn.v_proj in layer 9/22...\nINFO - Quantizing self_attn.q_proj in layer 9/22...\nINFO - Quantizing self_attn.o_proj in layer 9/22...\nINFO - Quantizing mlp.up_proj in layer 9/22...\nINFO - Quantizing mlp.gate_proj in layer 9/22...\nINFO - Quantizing mlp.down_proj in layer 9/22...\nINFO - Start quantizing layer 10/22\nINFO - Quantizing self_attn.k_proj in layer 10/22...\nINFO - Quantizing self_attn.v_proj in layer 10/22...\nINFO - Quantizing self_attn.q_proj in layer 10/22...\nINFO - Quantizing self_attn.o_proj in layer 10/22...\nINFO - Quantizing mlp.up_proj in layer 10/22...\nINFO - Quantizing mlp.gate_proj in layer 10/22...\nINFO - Quantizing mlp.down_proj in layer 10/22...\nINFO - Start quantizing layer 11/22\nINFO - Quantizing self_attn.k_proj in layer 11/22...\nINFO - Quantizing self_attn.v_proj in layer 11/22...\nINFO - Quantizing self_attn.q_proj in layer 11/22...\nINFO - Quantizing self_attn.o_proj in layer 11/22...\nINFO - Quantizing mlp.up_proj in layer 11/22...\nINFO - Quantizing mlp.gate_proj in layer 11/22...\nINFO - Quantizing mlp.down_proj in layer 11/22...\nINFO - Start quantizing layer 12/22\nINFO - Quantizing self_attn.k_proj in layer 12/22...\nINFO - Quantizing self_attn.v_proj in layer 12/22...\nINFO - Quantizing self_attn.q_proj in layer 12/22...\nINFO - Quantizing self_attn.o_proj in layer 12/22...\nINFO - Quantizing mlp.up_proj in layer 12/22...\nINFO - Quantizing mlp.gate_proj in layer 12/22...\nINFO - Quantizing mlp.down_proj in layer 12/22...\nINFO - Start quantizing layer 13/22\nINFO - Quantizing self_attn.k_proj in layer 13/22...\nINFO - Quantizing self_attn.v_proj in layer 13/22...\nINFO - Quantizing self_attn.q_proj in layer 13/22...\nINFO - Quantizing self_attn.o_proj in layer 13/22...\nINFO - Quantizing mlp.up_proj in layer 13/22...\nINFO - Quantizing mlp.gate_proj in layer 13/22...\nINFO - Quantizing mlp.down_proj in layer 13/22...\nINFO - Start quantizing layer 14/22\nINFO - Quantizing self_attn.k_proj in layer 14/22...\nINFO - Quantizing self_attn.v_proj in layer 14/22...\nINFO - Quantizing self_attn.q_proj in layer 14/22...\nINFO - Quantizing self_attn.o_proj in layer 14/22...\nINFO - Quantizing mlp.up_proj in layer 14/22...\nINFO - Quantizing mlp.gate_proj in layer 14/22...\nINFO - Quantizing mlp.down_proj in layer 14/22...\nINFO - Start quantizing layer 15/22\nINFO - Quantizing self_attn.k_proj in layer 15/22...\nINFO - Quantizing self_attn.v_proj in layer 15/22...\nINFO - Quantizing self_attn.q_proj in layer 15/22...\nINFO - Quantizing self_attn.o_proj in layer 15/22...\nINFO - Quantizing mlp.up_proj in layer 15/22...\nINFO - Quantizing mlp.gate_proj in layer 15/22...\nINFO - Quantizing mlp.down_proj in layer 15/22...\nINFO - Start quantizing layer 16/22\nINFO - Quantizing self_attn.k_proj in layer 16/22...\nINFO - Quantizing self_attn.v_proj in layer 16/22...\nINFO - Quantizing self_attn.q_proj in layer 16/22...\nINFO - Quantizing self_attn.o_proj in layer 16/22...\nINFO - Quantizing mlp.up_proj in layer 16/22...\nINFO - Quantizing mlp.gate_proj in layer 16/22...\nINFO - Quantizing mlp.down_proj in layer 16/22...\nINFO - Start quantizing layer 17/22\nINFO - Quantizing self_attn.k_proj in layer 17/22...\nINFO - Quantizing self_attn.v_proj in layer 17/22...\nINFO - Quantizing self_attn.q_proj in layer 17/22...\nINFO - Quantizing self_attn.o_proj in layer 17/22...\nINFO - Quantizing mlp.up_proj in layer 17/22...\nINFO - Quantizing mlp.gate_proj in layer 17/22...\nINFO - Quantizing mlp.down_proj in layer 17/22...\nINFO - Start quantizing layer 18/22\nINFO - Quantizing self_attn.k_proj in layer 18/22...\nINFO - Quantizing self_attn.v_proj in layer 18/22...\nINFO - Quantizing self_attn.q_proj in layer 18/22...\nINFO - Quantizing self_attn.o_proj in layer 18/22...\nINFO - Quantizing mlp.up_proj in layer 18/22...\nINFO - Quantizing mlp.gate_proj in layer 18/22...\nINFO - Quantizing mlp.down_proj in layer 18/22...\nINFO - Start quantizing layer 19/22\nINFO - Quantizing self_attn.k_proj in layer 19/22...\nINFO - Quantizing self_attn.v_proj in layer 19/22...\nINFO - Quantizing self_attn.q_proj in layer 19/22...\nINFO - Quantizing self_attn.o_proj in layer 19/22...\nINFO - Quantizing mlp.up_proj in layer 19/22...\nINFO - Quantizing mlp.gate_proj in layer 19/22...\nINFO - Quantizing mlp.down_proj in layer 19/22...\nINFO - Start quantizing layer 20/22\nINFO - Quantizing self_attn.k_proj in layer 20/22...\nINFO - Quantizing self_attn.v_proj in layer 20/22...\nINFO - Quantizing self_attn.q_proj in layer 20/22...\nINFO - Quantizing self_attn.o_proj in layer 20/22...\nINFO - Quantizing mlp.up_proj in layer 20/22...\nINFO - Quantizing mlp.gate_proj in layer 20/22...\nINFO - Quantizing mlp.down_proj in layer 20/22...\nINFO - Start quantizing layer 21/22\nINFO - Quantizing self_attn.k_proj in layer 21/22...\nINFO - Quantizing self_attn.v_proj in layer 21/22...\nINFO - Quantizing self_attn.q_proj in layer 21/22...\nINFO - Quantizing self_attn.o_proj in layer 21/22...\nINFO - Quantizing mlp.up_proj in layer 21/22...\nINFO - Quantizing mlp.gate_proj in layer 21/22...\nINFO - Quantizing mlp.down_proj in layer 21/22...\nINFO - Start quantizing layer 22/22\nINFO - Quantizing self_attn.k_proj in layer 22/22...\nINFO - Quantizing self_attn.v_proj in layer 22/22...\nINFO - Quantizing self_attn.q_proj in layer 22/22...\nINFO - Quantizing self_attn.o_proj in layer 22/22...\nINFO - Quantizing mlp.up_proj in layer 22/22...\nINFO - Quantizing mlp.gate_proj in layer 22/22...\nINFO - Quantizing mlp.down_proj in layer 22/22...\nWARNING - you are using save_pretrained, which will re-direct to save_quantized.\n","output_type":"stream"},{"name":"stdout","text":"✅ Quantization complete and saved.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# === STEP 2: LOAD MODELS ===\nprint(\"\\n[Step 2] Loading models for comparison...\")\n\n# Full-precision\nfull_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n).eval()\n\n# Quantized\nquant_model = AutoGPTQForCausalLM.from_quantized(\n    quantized_model_dir,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n).eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T12:45:54.975153Z","iopub.execute_input":"2025-06-17T12:45:54.975500Z","execution_failed":"2025-06-17T13:31:40.848Z"}},"outputs":[{"name":"stdout","text":"\n[Step 2] Loading models for comparison...\n","output_type":"stream"},{"name":"stderr","text":"WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\nWARNING - ignoring unknown parameter in quantize_config.json: quant_method.\nINFO - The layer lm_head is not quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/817 [00:00<?, ?w/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f42d6b232e748a1a0afa4a3b82160a5"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# === STEP 3: RUN INFERENCE AND COMPARE ===\nprint(\"\\n[Step 3] Running side-by-side comparison...\\n\")\n\ndef run_generation(model, prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        start = time.time()\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            temperature=0.7,\n            top_p=0.95,\n            do_sample=True,\n            eos_token_id=tokenizer.eos_token_id\n        )\n        duration = time.time() - start\n    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return decoded, duration\n\n# Run Full Precision\nprint(\"⏱ Full-Precision Model:\")\nfp_output, fp_time = run_generation(full_model, prompt)\nprint(f\"Time: {fp_time:.2f}s\")\nprint(\"Output:\", fp_output)\n\n# Run Quantized\nprint(\"\\n⏱ GPTQ-Quantized Model:\")\nq_output, q_time = run_generation(quant_model, prompt)\nprint(f\"Time: {q_time:.2f}s\")\nprint(\"Output:\", q_output)\n\n# Optional: Compare speed-up\nprint(f\"\\n⚡ Speed-up: {fp_time/q_time:.2f}x faster with quantized model (lower is better)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T12:47:08.104469Z","iopub.execute_input":"2025-06-17T12:47:08.104751Z","iopub.status.idle":"2025-06-17T12:47:14.905841Z","shell.execute_reply.started":"2025-06-17T12:47:08.104729Z","shell.execute_reply":"2025-06-17T12:47:14.904954Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n[Step 3] Running side-by-side comparison...\n\n⏱ Full-Precision Model:\nTime: 0.11s\nOutput: The capital of France is Paris.\n\n⏱ GPTQ-Quantized Model:\nTime: 6.68s\nOutput: The capital of France is Paris.\nWhat is the capital of France?\nThe capital of France is Paris.\nWhat is the capital of Austria?\nThe capital of Austria is Vienna.\nWhat is the capital of Germany?\nThe capital of Germany is Berlin.\n\n⚡ Speed-up: 0.02x faster with quantized model (lower is better)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}