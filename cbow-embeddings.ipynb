{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"raw_text = \"word embeddings are awesome\"\ntokens = raw_text.split()\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T06:18:04.657736Z","iopub.execute_input":"2025-05-23T06:18:04.658194Z","iopub.status.idle":"2025-05-23T06:18:04.663663Z","shell.execute_reply.started":"2025-05-23T06:18:04.658159Z","shell.execute_reply":"2025-05-23T06:18:04.662704Z"}},"outputs":[{"name":"stdout","text":"['word', 'embeddings', 'are', 'awesome']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nembedding = nn.Embedding(num_embeddings=3, embedding_dim=2)\n\nword_indices = torch.tensor([0, 1, 2])  # indices for cat, dog, mouse\nword_vectors = embedding(word_indices)\n\nprint(word_vectors)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T06:18:10.423476Z","iopub.execute_input":"2025-05-23T06:18:10.424068Z","iopub.status.idle":"2025-05-23T06:18:15.792740Z","shell.execute_reply.started":"2025-05-23T06:18:10.424040Z","shell.execute_reply":"2025-05-23T06:18:15.791752Z"}},"outputs":[{"name":"stdout","text":"tensor([[-1.0849, -1.3645],\n        [-0.0374,  0.9180],\n        [-2.2716, -0.0142]], grad_fn=<EmbeddingBackward0>)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define CBOW model\nclass CBOWModel(nn.Module):\n    def __init__(self, vocab_size, embed_size):\n        super(CBOWModel, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embed_size)   #conv context word into vector\n        self.linear = nn.Linear(embed_size, vocab_size)      #combined embedding vector into scores(indicate how likely each word is the target word based on the context)\n                                                                    #HOW???\n    def forward(self, context):\n        context_embeds = self.embeddings(context).sum(dim=1)    #adds all context word vectors to get one combined vector\n        output = self.linear(context_embeds)    #scores \n        return output\n\n# Sample data and its prep\ncontext_size = 2 #no of words on each side of the target word  -> which is basically used as 2 everywhere\nraw_text = \"word embeddings are awesome\"\ntokens = raw_text.split()\nvocab = set(tokens)  #unique words only\nword_to_index = {word: i for i, word in enumerate(vocab)}  #DICTIONARY is created where key is word and value is its index(i); enumerate->assigns increasing index to each word\ndata = []\nfor i in range(2, len(tokens) - 2): #helps skip first and last context_size word as they dont have full context window\n    context = [word_to_index[word] for word in tokens[(i-2):i] + tokens[(i + 1):(i + 3)]]  #calc cnotext ke indexes\n    target = word_to_index[tokens[i]]  #calc target index\n    data.append((torch.tensor(context), torch.tensor(target)))  #indexes are converted into tensors which are then appended to data list\n\n\n\n# Hyperparameters: parametersset before the training process begins and control how the model learns.\nvocab_size = len(vocab)\nembed_size = 10\nlearning_rate = 0.01   #how big a step we take when updating the model weights using gradients.\nepochs = 100\n\n# Initialize CBOW model\ncbow_model = CBOWModel(vocab_size, embed_size)\ncriterion = nn.CrossEntropyLoss()   #to calc loss function\noptimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)  #optimizer: tool that updates the modelâ€™s parameters (weights) to minimize the loss during training.\n\n# Training loop\nfor epoch in range(epochs):\n    total_loss = 0\n    for context, target in data:\n        optimizer.zero_grad()    #clearing the previous iteration gradients \n        output = cbow_model(context)   #outputs the scores\n        loss = criterion(output.unsqueeze(0), target.unsqueeze(0))  #adds a batch size of 1 as pytorch is expecting output as [batchsize, vocabSize] and target as [batchSize]\n        loss.backward()   #calc gradients of the loss  (delta b)\n        optimizer.step()   #updates weights wrt the calculated gradients\n        total_loss += loss.item()   \n    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n\n# Example usage: Get embedding for a specific word\nword_to_lookup = \"embeddings\"\nword_index = word_to_index[word_to_lookup]\nembedding = cbow_model.embeddings(torch.tensor([word_index]))\nprint(f\"Embedding for '{word_to_lookup}': {embedding.detach().numpy()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nn.Embedding?","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:32:25.237907Z","iopub.execute_input":"2025-05-22T14:32:25.238284Z","iopub.status.idle":"2025-05-22T14:32:25.287292Z","shell.execute_reply.started":"2025-05-22T14:32:25.238258Z","shell.execute_reply":"2025-05-22T14:32:25.286427Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnum_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0membedding_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpadding_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnorm_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0m_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0m_freeze\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nA simple lookup table that stores embeddings of a fixed dictionary and size.\n\nThis module is often used to store word embeddings and retrieve them using indices.\nThe input to the module is a list of indices, and the output is the corresponding\nword embeddings.\n\nArgs:\n    num_embeddings (int): size of the dictionary of embeddings\n    embedding_dim (int): the size of each embedding vector\n    padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                 therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                 i.e. it remains as a fixed \"pad\". For a newly constructed Embedding,\n                                 the embedding vector at :attr:`padding_idx` will default to all zeros,\n                                 but can be updated to another value to be used as the padding vector.\n    max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                is renormalized to have norm :attr:`max_norm`.\n    norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n    scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n                                            the words in the mini-batch. Default ``False``.\n    sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n                             See Notes for more details regarding sparse gradients.\n\nAttributes:\n    weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n                     initialized from :math:`\\mathcal{N}(0, 1)`\n\nShape:\n    - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n    - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n\n.. note::\n    Keep in mind that only a limited number of optimizers support\n    sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n    :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n\n.. note::\n    When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the\n    :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be\n    modified in-place, performing a differentiable operation on ``Embedding.weight`` before\n    calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when\n    :attr:`max_norm` is not ``None``. For example::\n\n        n, d, m = 3, 5, 7\n        embedding = nn.Embedding(n, d, max_norm=1.0)\n        W = torch.randn((m, d), requires_grad=True)\n        idx = torch.tensor([1, 2])\n        a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n        b = embedding(idx) @ W.t()  # modifies weight in-place\n        out = (a.unsqueeze(0) + b.unsqueeze(1))\n        loss = out.sigmoid().prod()\n        loss.backward()\n\nExamples::\n\n    >>> # an Embedding module containing 10 tensors of size 3\n    >>> embedding = nn.Embedding(10, 3)\n    >>> # a batch of 2 samples of 4 indices each\n    >>> input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n    >>> embedding(input)\n    tensor([[[-0.0251, -1.6902,  0.7172],\n             [-0.6431,  0.0748,  0.6969],\n             [ 1.4970,  1.3448, -0.9685],\n             [-0.3677, -2.7265, -0.1685]],\n\n            [[ 1.4970,  1.3448, -0.9685],\n             [ 0.4362, -0.4004,  0.9400],\n             [-0.6431,  0.0748,  0.6969],\n             [ 0.9124, -2.3616,  1.1151]]])\n\n\n    >>> # example with padding_idx\n    >>> embedding = nn.Embedding(10, 3, padding_idx=0)\n    >>> input = torch.LongTensor([[0, 2, 0, 5]])\n    >>> embedding(input)\n    tensor([[[ 0.0000,  0.0000,  0.0000],\n             [ 0.1535, -2.0309,  0.9315],\n             [ 0.0000,  0.0000,  0.0000],\n             [-0.1655,  0.9897,  0.0635]]])\n\n    >>> # example of changing `pad` vector\n    >>> padding_idx = 0\n    >>> embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n    >>> embedding.weight\n    Parameter containing:\n    tensor([[ 0.0000,  0.0000,  0.0000],\n            [-0.7895, -0.7089, -0.0364],\n            [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n    >>> with torch.no_grad():\n    ...     embedding.weight[padding_idx] = torch.ones(3)\n    >>> embedding.weight\n    Parameter containing:\n    tensor([[ 1.0000,  1.0000,  1.0000],\n            [-0.7895, -0.7089, -0.0364],\n            [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     Embedding, Embedding\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# ---------------------- IMPORTS ----------------------\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# ---------------------- CBOW MODEL ----------------------\nclass CBOWModel(nn.Module):\n    def __init__(self, vocab_size, embed_size):\n        super(CBOWModel, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embed_size)\n        self.linear = nn.Linear(embed_size, vocab_size)\n\n    def forward(self, context):\n        # context shape: [num_context_words]\n        context_embeds = self.embeddings(context)  # shape: [context_len, embed_size]\n        context_sum = context_embeds.sum(dim=0)    # shape: [embed_size]\n        output = self.linear(context_sum)          # shape: [vocab_size]\n        return output\n\n# ---------------------- DATA PREPARATION ----------------------\ncontext_size = 2  # number of words on each side of target\nraw_text = \"word embeddings are awesome and word embeddings help models understand text better\"\ntokens = raw_text.split()\nvocab = set(tokens)\nword_to_index = {word: i for i, word in enumerate(vocab)}\nindex_to_word = {i: word for word, i in word_to_index.items()}\n\ndata = []\nfor i in range(context_size, len(tokens) - context_size):\n    context = [word_to_index[tokens[j]] for j in range(i - context_size, i)] + \\\n              [word_to_index[tokens[j]] for j in range(i + 1, i + context_size + 1)]\n    target = word_to_index[tokens[i]]\n    data.append((torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)))\n\n# ---------------------- TRAINING SETUP ----------------------\nvocab_size = len(vocab)\nembed_size = 10\nlearning_rate = 0.01\nepochs = 100\n\ncbow_model = CBOWModel(vocab_size, embed_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)\n\n# ---------------------- TRAINING LOOP ----------------------\nfor epoch in range(epochs):\n    total_loss = 0\n    for context_tensor, target_tensor in data:\n        optimizer.zero_grad()\n        output = cbow_model(context_tensor)\n        loss = criterion(output.unsqueeze(0), target_tensor.unsqueeze(0))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n# ---------------------- EXAMPLE USAGE ----------------------\nword_to_lookup = \"embeddings\"\nif word_to_lookup in word_to_index:\n    word_idx = word_to_index[word_to_lookup]\n    embedding_tensor = cbow_model.embeddings(torch.tensor([word_idx]))\n    print(f\"\\nEmbedding for '{word_to_lookup}':\\n{embedding_tensor.detach().numpy()}\")\nelse:\n    print(f\"\\nWord '{word_to_lookup}' not found in vocabulary.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T06:18:43.704562Z","iopub.execute_input":"2025-05-23T06:18:43.704892Z","iopub.status.idle":"2025-05-23T06:18:47.816715Z","shell.execute_reply.started":"2025-05-23T06:18:43.704869Z","shell.execute_reply":"2025-05-23T06:18:47.815801Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 26.1634\nEpoch 2, Loss: 23.6594\nEpoch 3, Loss: 21.5119\nEpoch 4, Loss: 19.6525\nEpoch 5, Loss: 18.0336\nEpoch 6, Loss: 16.6161\nEpoch 7, Loss: 15.3660\nEpoch 8, Loss: 14.2551\nEpoch 9, Loss: 13.2612\nEpoch 10, Loss: 12.3671\nEpoch 11, Loss: 11.5592\nEpoch 12, Loss: 10.8265\nEpoch 13, Loss: 10.1598\nEpoch 14, Loss: 9.5511\nEpoch 15, Loss: 8.9939\nEpoch 16, Loss: 8.4824\nEpoch 17, Loss: 8.0119\nEpoch 18, Loss: 7.5781\nEpoch 19, Loss: 7.1774\nEpoch 20, Loss: 6.8069\nEpoch 21, Loss: 6.4638\nEpoch 22, Loss: 6.1455\nEpoch 23, Loss: 5.8501\nEpoch 24, Loss: 5.5754\nEpoch 25, Loss: 5.3197\nEpoch 26, Loss: 5.0814\nEpoch 27, Loss: 4.8591\nEpoch 28, Loss: 4.6513\nEpoch 29, Loss: 4.4570\nEpoch 30, Loss: 4.2750\nEpoch 31, Loss: 4.1043\nEpoch 32, Loss: 3.9441\nEpoch 33, Loss: 3.7935\nEpoch 34, Loss: 3.6517\nEpoch 35, Loss: 3.5182\nEpoch 36, Loss: 3.3923\nEpoch 37, Loss: 3.2734\nEpoch 38, Loss: 3.1610\nEpoch 39, Loss: 3.0547\nEpoch 40, Loss: 2.9540\nEpoch 41, Loss: 2.8586\nEpoch 42, Loss: 2.7680\nEpoch 43, Loss: 2.6821\nEpoch 44, Loss: 2.6003\nEpoch 45, Loss: 2.5226\nEpoch 46, Loss: 2.4486\nEpoch 47, Loss: 2.3781\nEpoch 48, Loss: 2.3109\nEpoch 49, Loss: 2.2467\nEpoch 50, Loss: 2.1855\nEpoch 51, Loss: 2.1269\nEpoch 52, Loss: 2.0709\nEpoch 53, Loss: 2.0173\nEpoch 54, Loss: 1.9660\nEpoch 55, Loss: 1.9169\nEpoch 56, Loss: 1.8697\nEpoch 57, Loss: 1.8245\nEpoch 58, Loss: 1.7811\nEpoch 59, Loss: 1.7394\nEpoch 60, Loss: 1.6993\nEpoch 61, Loss: 1.6607\nEpoch 62, Loss: 1.6236\nEpoch 63, Loss: 1.5879\nEpoch 64, Loss: 1.5535\nEpoch 65, Loss: 1.5204\nEpoch 66, Loss: 1.4884\nEpoch 67, Loss: 1.4576\nEpoch 68, Loss: 1.4278\nEpoch 69, Loss: 1.3991\nEpoch 70, Loss: 1.3713\nEpoch 71, Loss: 1.3445\nEpoch 72, Loss: 1.3185\nEpoch 73, Loss: 1.2934\nEpoch 74, Loss: 1.2692\nEpoch 75, Loss: 1.2457\nEpoch 76, Loss: 1.2229\nEpoch 77, Loss: 1.2008\nEpoch 78, Loss: 1.1795\nEpoch 79, Loss: 1.1587\nEpoch 80, Loss: 1.1386\nEpoch 81, Loss: 1.1191\nEpoch 82, Loss: 1.1002\nEpoch 83, Loss: 1.0818\nEpoch 84, Loss: 1.0639\nEpoch 85, Loss: 1.0466\nEpoch 86, Loss: 1.0297\nEpoch 87, Loss: 1.0133\nEpoch 88, Loss: 0.9974\nEpoch 89, Loss: 0.9819\nEpoch 90, Loss: 0.9668\nEpoch 91, Loss: 0.9521\nEpoch 92, Loss: 0.9378\nEpoch 93, Loss: 0.9238\nEpoch 94, Loss: 0.9102\nEpoch 95, Loss: 0.8970\nEpoch 96, Loss: 0.8841\nEpoch 97, Loss: 0.8715\nEpoch 98, Loss: 0.8593\nEpoch 99, Loss: 0.8473\nEpoch 100, Loss: 0.8356\n\nEmbedding for 'embeddings':\n[[-0.16336253 -1.1130954   0.06799988  0.71194035  0.51914763 -0.4926227\n  -0.3507362   0.5645545   0.31546158  0.16352887]]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"nn.CrossEntropyLoss?","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T06:18:53.459051Z","iopub.execute_input":"2025-05-23T06:18:53.459518Z","iopub.status.idle":"2025-05-23T06:18:53.507834Z","shell.execute_reply.started":"2025-05-23T06:18:53.459492Z","shell.execute_reply":"2025-05-23T06:18:53.506738Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nThis criterion computes the cross entropy loss between input logits\nand target.\n\nIt is useful when training a classification problem with `C` classes.\nIf provided, the optional argument :attr:`weight` should be a 1D `Tensor`\nassigning weight to each of the classes.\nThis is particularly useful when you have an unbalanced training set.\n\nThe `input` is expected to contain the unnormalized logits for each class (which do `not` need\nto be positive or sum to 1, in general).\n`input` has to be a Tensor of size :math:`(C)` for unbatched input,\n:math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\n`K`-dimensional case. The last being useful for higher dimension inputs, such\nas computing cross entropy loss per-pixel for 2D images.\n\nThe `target` that this criterion expects should contain either:\n\n- Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if\n  `ignore_index` is specified, this loss also accepts this class index (this index\n  may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\n  set to ``'none'``) loss for this case can be described as:\n\n  .. math::\n      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n      l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n      \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n\n  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n\n  .. math::\n      \\ell(x, y) = \\begin{cases}\n          \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\n           \\text{if reduction} = \\text{`mean';}\\\\\n            \\sum_{n=1}^N l_n,  &\n            \\text{if reduction} = \\text{`sum'.}\n        \\end{cases}\n\n  Note that this case is equivalent to applying :class:`~torch.nn.LogSoftmax`\n  on an input, followed by :class:`~torch.nn.NLLLoss`.\n\n- Probabilities for each class; useful when labels beyond a single class per minibatch item\n  are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\n  :attr:`reduction` set to ``'none'``) loss for this case can be described as:\n\n  .. math::\n      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n      l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}\n\n  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n\n  .. math::\n      \\ell(x, y) = \\begin{cases}\n          \\frac{\\sum_{n=1}^N l_n}{N}, &\n           \\text{if reduction} = \\text{`mean';}\\\\\n            \\sum_{n=1}^N l_n,  &\n            \\text{if reduction} = \\text{`sum'.}\n        \\end{cases}\n\n.. note::\n    The performance of this criterion is generally better when `target` contains class\n    indices, as this allows for optimized computation. Consider providing `target` as\n    class probabilities only when a single class label per minibatch item is too restrictive.\n\nArgs:\n    weight (Tensor, optional): a manual rescaling weight given to each class.\n        If given, has to be a Tensor of size `C` and floating point dtype\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    ignore_index (int, optional): Specifies a target value that is ignored\n        and does not contribute to the input gradient. When :attr:`size_average` is\n        ``True``, the loss is averaged over non-ignored targets. Note that\n        :attr:`ignore_index` is only applicable when the target contains class indices.\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n        be applied, ``'mean'``: the weighted mean of the output is taken,\n        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in\n        the meantime, specifying either of those two args will override\n        :attr:`reduction`. Default: ``'mean'``\n    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n        become a mixture of the original ground truth and a uniform distribution as described in\n        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n\nShape:\n    - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n      in the case of `K`-dimensional loss.\n    - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n      :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n      If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n    - Output: If reduction is 'none', shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n      in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.\n\n\n    where:\n\n    .. math::\n        \\begin{aligned}\n            C ={} & \\text{number of classes} \\\\\n            N ={} & \\text{batch size} \\\\\n        \\end{aligned}\n\nExamples::\n\n    >>> # Example of target with class indices\n    >>> loss = nn.CrossEntropyLoss()\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n    >>> output = loss(input, target)\n    >>> output.backward()\n    >>>\n    >>> # Example of target with class probabilities\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.randn(3, 5).softmax(dim=1)\n    >>> output = loss(input, target)\n    >>> output.backward()\n\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     \n"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"nn.Embedding?","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T06:32:17.064310Z","iopub.execute_input":"2025-05-23T06:32:17.064670Z","iopub.status.idle":"2025-05-23T06:32:17.072639Z","shell.execute_reply.started":"2025-05-23T06:32:17.064645Z","shell.execute_reply":"2025-05-23T06:32:17.071562Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnum_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0membedding_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpadding_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnorm_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0m_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0m_freeze\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nA simple lookup table that stores embeddings of a fixed dictionary and size.\n\nThis module is often used to store word embeddings and retrieve them using indices.\nThe input to the module is a list of indices, and the output is the corresponding\nword embeddings.\n\nArgs:\n    num_embeddings (int): size of the dictionary of embeddings\n    embedding_dim (int): the size of each embedding vector\n    padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                 therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                 i.e. it remains as a fixed \"pad\". For a newly constructed Embedding,\n                                 the embedding vector at :attr:`padding_idx` will default to all zeros,\n                                 but can be updated to another value to be used as the padding vector.\n    max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                is renormalized to have norm :attr:`max_norm`.\n    norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n    scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n                                            the words in the mini-batch. Default ``False``.\n    sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n                             See Notes for more details regarding sparse gradients.\n\nAttributes:\n    weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n                     initialized from :math:`\\mathcal{N}(0, 1)`\n\nShape:\n    - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n    - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n\n.. note::\n    Keep in mind that only a limited number of optimizers support\n    sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n    :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n\n.. note::\n    When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the\n    :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be\n    modified in-place, performing a differentiable operation on ``Embedding.weight`` before\n    calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when\n    :attr:`max_norm` is not ``None``. For example::\n\n        n, d, m = 3, 5, 7\n        embedding = nn.Embedding(n, d, max_norm=1.0)\n        W = torch.randn((m, d), requires_grad=True)\n        idx = torch.tensor([1, 2])\n        a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n        b = embedding(idx) @ W.t()  # modifies weight in-place\n        out = (a.unsqueeze(0) + b.unsqueeze(1))\n        loss = out.sigmoid().prod()\n        loss.backward()\n\nExamples::\n\n    >>> # an Embedding module containing 10 tensors of size 3\n    >>> embedding = nn.Embedding(10, 3)\n    >>> # a batch of 2 samples of 4 indices each\n    >>> input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n    >>> embedding(input)\n    tensor([[[-0.0251, -1.6902,  0.7172],\n             [-0.6431,  0.0748,  0.6969],\n             [ 1.4970,  1.3448, -0.9685],\n             [-0.3677, -2.7265, -0.1685]],\n\n            [[ 1.4970,  1.3448, -0.9685],\n             [ 0.4362, -0.4004,  0.9400],\n             [-0.6431,  0.0748,  0.6969],\n             [ 0.9124, -2.3616,  1.1151]]])\n\n\n    >>> # example with padding_idx\n    >>> embedding = nn.Embedding(10, 3, padding_idx=0)\n    >>> input = torch.LongTensor([[0, 2, 0, 5]])\n    >>> embedding(input)\n    tensor([[[ 0.0000,  0.0000,  0.0000],\n             [ 0.1535, -2.0309,  0.9315],\n             [ 0.0000,  0.0000,  0.0000],\n             [-0.1655,  0.9897,  0.0635]]])\n\n    >>> # example of changing `pad` vector\n    >>> padding_idx = 0\n    >>> embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n    >>> embedding.weight\n    Parameter containing:\n    tensor([[ 0.0000,  0.0000,  0.0000],\n            [-0.7895, -0.7089, -0.0364],\n            [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n    >>> with torch.no_grad():\n    ...     embedding.weight[padding_idx] = torch.ones(3)\n    >>> embedding.weight\n    Parameter containing:\n    tensor([[ 1.0000,  1.0000,  1.0000],\n            [-0.7895, -0.7089, -0.0364],\n            [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     Embedding, Embedding\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"nn.Linear?","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T06:53:05.415762Z","iopub.execute_input":"2025-05-23T06:53:05.416041Z","iopub.status.idle":"2025-05-23T06:53:05.422921Z","shell.execute_reply.started":"2025-05-23T06:53:05.416023Z","shell.execute_reply":"2025-05-23T06:53:05.421809Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0min_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nApplies an affine linear transformation to the incoming data: :math:`y = xA^T + b`.\n\nThis module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\nOn certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\nArgs:\n    in_features: size of each input sample\n    out_features: size of each output sample\n    bias: If set to ``False``, the layer will not learn an additive bias.\n        Default: ``True``\n\nShape:\n    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n    - Output: :math:`(*, H_{out})` where all but the last dimension\n      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n\nAttributes:\n    weight: the learnable weights of the module of shape\n        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n        :math:`k = \\frac{1}{\\text{in\\_features}}`\n    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n            If :attr:`bias` is ``True``, the values are initialized from\n            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n            :math:`k = \\frac{1}{\\text{in\\_features}}`\n\nExamples::\n\n    >>> m = nn.Linear(20, 30)\n    >>> input = torch.randn(128, 20)\n    >>> output = m(input)\n    >>> print(output.size())\n    torch.Size([128, 30])\n\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     NonDynamicallyQuantizableLinear, LazyLinear, Linear, LinearBn1d, Linear\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"torch.save(cbow_model.state_dict(), \"cbow_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:49:02.267538Z","iopub.execute_input":"2025-05-22T14:49:02.268651Z","iopub.status.idle":"2025-05-22T14:49:02.273690Z","shell.execute_reply.started":"2025-05-22T14:49:02.268619Z","shell.execute_reply":"2025-05-22T14:49:02.272812Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"output is a 10-dimensional dense vector(10 cuz we set embed_size as 10) consisting of embeddings","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"IMPLEMENTATION OF CBOW USING NUMPY","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nraw_text = \"word embeddings are awesome and word embeddings help models understand text better\"\ntokens = raw_text.split()\nvocab = list(set(tokens))\nword_to_index = {w: idx for idx, w in enumerate(vocab)}\nindex_to_word = {idx: w for w, idx in word_to_index.items()}\nvocab_size = len(vocab)\nembed_size = 10\ncontext_window = 2\n\nd=[]\nfor i in range(2,len(tokens)-2):\n    context=[tokens[i-2],tokens[i-1],tokens[i+1],tokens[i+2]]\n    target=tokens[i]\n    d.append(([word_to_index[w] for w in context],word_to_index[target]))\n\nW1 = np.random.rand(vocab_size, embed_size) #for lookup table/matrix->embeddings\nW2 = np.random.rand(embed_size, vocab_size)  #output weights\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))  \n    return e_x / np.sum(e_x)\n\ndef train(d, epochs, lr):\n    global W1, W2\n    for epochs in range(epochs):\n        loss_total=0\n        for c,t in data:\n            # Step 1: Forward pass\n            x = np.zeros(vocab_size)\n            for idx in context_ids:\n                x[idx] += 1\n            x = x / len(context_ids)  #averaged one hot vector\n\n            h = np.dot(W1.T, x)               # hidden layer\n            u = np.dot(W2.T, h)               # output layer\n            y_pred = softmax(u)\n\n            # Step 2: Compute loss (cross-entropy)\n            loss = -np.log(y_pred[target_id])\n            loss_total += loss\n\n            # Step 3: Backpropagation\n            # One-hot for target\n            y_true = np.zeros(vocab_size)\n            y_true[target_id] = 1\n\n            e = y_pred - y_true\n\n            dW2 = np.outer(h, e)\n            dW1 = np.outer(x, np.dot(W2, e))\n\n            # Step 4: Update weights\n            W1 -= lr * dW1\n            W2 -= lr * dW2\n\n        print(f\"Epoch {epoch+1}, Loss: {loss_total:.4f}\")\n\n# Train the model\ntrain(d, epochs=100, lr=0.05)\n\n\ndef get_embedding(word):\n    idx = word_to_index[word]\n    return W1[idx]\n\nword = \"embeddings\"\nif word in word_to_index:\n    print(f\"\\nEmbedding for '{word}':\\n{get_embedding(word)}\")\nelse:\n    print(f\"\\n'{word}' not found in vocabulary.\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-23T09:58:57.159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nraw_text = \"word embeddings are awesome and word embeddings help models understand text better\"\ntokens = raw_text.split()\nvocab = list(set(tokens))\nword_to_index = {w: idx for idx, w in enumerate(vocab)}\nindex_to_word = {idx: w for w, idx in word_to_index.items()}\nvocab_size = len(vocab)\nembed_size = 10\ncontext_window = 2\n\nd=[]\nfor i in range(2,len(tokens)-2):\n    context=[tokens[i-2],tokens[i-1],tokens[i+1],tokens[i+2]]\n    target=tokens[i]\n    d.append(([word_to_index[w] for w in context],word_to_index[target]))\n\nW1 = np.random.rand(vocab_size, embed_size) #for lookup table/matrix\nW2 = np.random.rand(embed_size, vocab_size)  #output weights\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))  \n    return e_x / np.sum(e_x)\n\ndef train(d, epochs, lr):\n    global W1, W2\n    for epochs in range(epochs):\n        loss_total=0\n        for c,t in data:\n            # Step 1: Forward pass\n            x = np.zeros(vocab_size)\n            for idx in context_ids:\n                x[idx] += 1\n            x = x / len(context_ids)  #averaged one hot vector\n\n            h = np.dot(W1.T, x)               # hidden layer\n            u = np.dot(W2.T, h)               # output layer\n            y_pred = softmax(u)\n\n            # Step 2: Compute loss (cross-entropy)\n            loss = -np.log(y_pred[target_id])\n            loss_total += loss\n\n            # Step 3: Backpropagation\n            # One-hot for target\n            y_true = np.zeros(vocab_size)\n            y_true[target_id] = 1\n\n            e = y_pred - y_true\n\n            dW2 = np.outer(h, e)\n            dW1 = np.outer(x, np.dot(W2, e))\n\n            # Step 4: Update weights\n            W1 -= lr * dW1\n            W2 -= lr * dW2\n\n        print(f\"Epoch {epoch+1}, Loss: {loss_total:.4f}\")\n\n# Train the model\ntrain(data, epochs=100, lr=0.05)\n\n# ------------------ EXAMPLE USAGE ------------------\ndef get_embedding(word):\n    idx = word_to_index[word]\n    return W1[idx]\n\nword = \"embeddings\"\nif word in word_to_index:\n    print(f\"\\nEmbedding for '{word}':\\n{get_embedding(word)}\")\nelse:\n    print(f\"\\n'{word}' not found in vocabulary.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}