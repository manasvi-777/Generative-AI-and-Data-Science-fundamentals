{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER-DECODER MHA WITH MASKING MANUAL IMPLEMENTATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to fetch word embeddings with help of token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.shape[1]].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_table(sentence, vocab_embeds, embedding):\n",
    "\n",
    "    for i in range(sentence.size(0)):\n",
    "        for j in range(sentence.size(1)):\n",
    "            \n",
    "            # Get the index for the current word token index in the sequence\n",
    "            word_index = sentence[i, j].item()\n",
    "\n",
    "            if word_index < 0 or word_index >= vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            embedding[i, j, :] = vocab_embeds[word_index, :]\n",
    "\n",
    "            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_outputs(src_sentence,  tgt_sentence, max_seq_len, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_sentence.size(0), src_sentence.size(1), d_model)\n",
    "    print(\"Source sentence embedding\")\n",
    "    src_embedding =  look_up_table(src_sentence, src_vocab_embeds, src_embedding)\n",
    "    print(src_embedding.shape)\n",
    "\n",
    "    tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n",
    "\n",
    "    tgt_embedding = torch.zeros(tgt_sentence.size(0), tgt_sentence.size(1), d_model)\n",
    "\n",
    "\n",
    "    print(\"Target sentence embedding\")\n",
    "    tgt_embedding =  look_up_table(tgt_sentence, tgt_vocab_embeds, tgt_embedding)\n",
    "\n",
    "    pe = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "    print(\"PE of src :\")\n",
    "    print(pe(src_sentence))\n",
    "    print()\n",
    "    print(\"PE of tgt :\")\n",
    "    print(pe(tgt_sentence))\n",
    "    print()\n",
    "\n",
    "    pe_src_embeds = src_embedding + pe(src_sentence)\n",
    "\n",
    "    pe_tgt_embeds = tgt_embedding + pe(tgt_sentence)\n",
    "\n",
    "    print(\"PE source embeddings : \\n\")\n",
    "    print(pe_src_embeds)\n",
    "    print()\n",
    "\n",
    "    print(\"PE target embeddings : \\n\")\n",
    "    print(pe_tgt_embeds)\n",
    "    print()\n",
    "\n",
    "    return pe_src_embeds, pe_tgt_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder function to display the intermediate outputs and get the final outputs from the encoder\n",
    "\n",
    "### Self attention \n",
    "\n",
    "#### Functions to perform the attention calculation with Q,K and V matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atten_product_needs_wts_false(Q, V, K, bsz, head_dim, src_len, tgt_len, embed_dim, attn_mask, num_heads):\n",
    "\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim) -> (bsz, num_heads, tgt_len, head_dim)\n",
    "    Q1 = Q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "    K1 = K.view(bsz, num_heads, src_len, head_dim)\n",
    "    V1 = V.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "\n",
    "    L, S = Q1.size(-2), K1.size(-2)\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(Q1.size(-1)) \n",
    "    # scale_factor = 1\n",
    "    attn_bias = torch.zeros(L, S, dtype=Q1.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            # attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "            masked_tensor = attn_mask.float().masked_fill(attn_mask, float('-inf'))\n",
    "            masked_tensor = masked_tensor.masked_fill(~attn_mask, 0)\n",
    "            attn_mask = masked_tensor\n",
    "\n",
    "            print(\"Attnetion mask infunction = \")\n",
    "            print(attn_mask)\n",
    "            print()\n",
    "            \n",
    "            attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "            attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "    # print(\"Attnetion bias = \", attn_bias.shape)\n",
    "    # print(attn_bias)\n",
    "    # print()\n",
    "            \n",
    "\n",
    "    # (bsz, num_heads, tgt_len, head_dim) @ (bsz, num_heads, head_dim, tgt_len) -> (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = Q1 @ K1.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "    print(\"Attn weight = \", attn_weight)\n",
    "    print()\n",
    "\n",
    "    sum_last_dim = attn_weight.sum(dim=-1)\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "\n",
    "    print(attn_weight)\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) @ (bsz, num_heads, tgt_len, head_dim) -> (bsz, num_heads, tgt_len, head_dim) \n",
    "    attn_output = attn_weight @ V1\n",
    "\n",
    "    print(\"ATT = \", attn_output)\n",
    "\n",
    "    # print(\"Dot product attention  = \")\n",
    "    # print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    # print(attn_output.shape)\n",
    "    # print(bsz, tgt_len, embed_dim)\n",
    "    \n",
    "    # (bsz*tgt_len, embed_dim)\n",
    "    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "    print(\"Attention output = \")\n",
    "    print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    return attn_output\n",
    "\n",
    "\n",
    "\n",
    "def atten_product_needs_wts_true(Q, K, V, bsz, tgt_len, embed_dim, attn_mask):\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim)\n",
    "    \n",
    "    B, Nt, E = Q.shape\n",
    "\n",
    "    Q_scaled = Q / math.sqrt(E)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        temp_pdt_matrix = torch.baddbmm(attn_mask, Q_scaled, K.transpose(-2, -1))\n",
    "    else:\n",
    "        temp_pdt_matrix = torch.bmm(Q_scaled, K.transpose(-2, -1))\n",
    "\n",
    "    attn_wt_matrix = torch.nn.functional.softmax(temp_pdt_matrix, dim=-1)\n",
    "\n",
    "    attn_output = torch.bmm(attn_wt_matrix, V)\n",
    "\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "\n",
    "    sum_last_dim = attn_wt_matrix.sum(dim=-1)\n",
    "\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "\n",
    "    print(\"Encoder Attention output = \")\n",
    "    print(attn_output)\n",
    "    print()\n",
    "\n",
    "    return attn_output, attn_wt_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the Q,K,V matrices from the model's intialised weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(query, key, value ,W, b):\n",
    "\n",
    "    # embed_dim\n",
    "    E = query.size(-1)\n",
    "\n",
    "    if key is value:\n",
    "        if query is key:\n",
    "            \n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*num_heads, embed_dim).T -> (src_len, bsz, embed_dim*num_heads)\n",
    "            tempop1 = query@W.T\n",
    "\n",
    "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            tempop1 = tempop1.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return tempop1[0], tempop1[1], tempop1[2]\n",
    "        \n",
    "\n",
    "        else:\n",
    "\n",
    "            # (embed_dim*1, embed_dim)\n",
    "            # (embed_dim*2, embed_dim)\n",
    "            W_q, W_kv = W.split([E, E * 2])\n",
    "\n",
    "\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*1, embed_dim).T -> (src_len, bsz, embed_dim)\n",
    "            q_matmul = query@W_q.T\n",
    "\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*2, embed_dim).T -> (src_len, bsz, embed_dim*2)\n",
    "            kv_matmul = key@W_kv.T\n",
    "            print(kv_matmul)\n",
    "\n",
    "            kv_matmul = kv_matmul.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return q_matmul, kv_matmul[0], kv_matmul[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        W_q, W_k, W_v = W.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "\n",
    "\n",
    "        q_matmul = query@W_q.T\n",
    "        k_matmul = key@W_k.T\n",
    "        v_matmul = value@W_v.T\n",
    "\n",
    "        return q_matmul, k_matmul, v_matmul\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder block's self attention output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_attn_output(x, state_dict, layer_num, embed_dim, num_heads, need_weights = False, src_mask = None):\n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    query_enc = key_enc = value_enc = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "    \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "    \n",
    "    # (src_len, bsz, embed_dim)\n",
    "    Q_enc,K_enc,V_enc = get_qkv(query_enc, key_enc, value_enc ,W_enc, b_enc)\n",
    "\n",
    "    print(\"Q_enc_shape = \", Q_enc.shape)\n",
    "    print(\"K_enc_shape = \", K_enc.shape)\n",
    "    print(\"V_enc_shape = \", V_enc.shape)\n",
    "    print()\n",
    "    \n",
    "    # (1, src_len, bsz, embed_dim)\n",
    "    # Q_enc = Q_enc.unsqueeze(0)\n",
    "    # K_enc = K_enc.unsqueeze(0)\n",
    "    # V_enc = V_enc.unsqueeze(0)\n",
    "\n",
    "    # (1, src_len, bsz, embed_dim) -> ( bsz*num_heads, src_len , head_dim)\n",
    "    Q_enc = Q_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_enc = K_enc.reshape(K_enc.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_enc = V_enc.reshape(V_enc.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_enc_shape = \", Q_enc.shape)\n",
    "    print(\"K_enc_shape = \", K_enc.shape)\n",
    "    print(\"V_enc_shape = \", V_enc.shape)\n",
    "    print()\n",
    "\n",
    "\n",
    "    print(\"Q_enc_{} = \".format(layer_num))\n",
    "    print(Q_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"K_enc_{} = \".format(layer_num))\n",
    "    print(K_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"V_enc_{} = \".format(layer_num))\n",
    "    print(V_enc)\n",
    "    print()\n",
    "\n",
    "\n",
    "    src_len = K_enc.size(1)\n",
    "\n",
    "    attn_mask = src_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output = atten_product_needs_wts_false(Q_enc, V_enc, K_enc, bsz, head_dim, src_len, tgt_len, embed_dim, attn_mask, num_heads=num_heads)\n",
    "\n",
    "        return attn_output, src_len, head_dim, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_enc_output,attn_wt_matrix_enc = atten_product_needs_wts_true(Q_enc, K_enc, V_enc, bsz, tgt_len, embed_dim,  attn_mask)\n",
    "\n",
    "        return attn_enc_output, src_len, head_dim, attn_wt_matrix_enc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post self attention in encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to perform the linear layer calculations after encoder's self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_post_attn_output(x, attn_enc_output, state_dict, layer_num, bsz, tgt_len):\n",
    "\n",
    "    # (bsz*src_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*src_len , embed_dim)\n",
    "\n",
    "    op_enc_1 = torch.matmul(attn_enc_output, state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "    # (bsz*src_len , embed_dim) -> (src_len, bsz, embed_dim)\n",
    "    attn_enc_output = op_enc_1.view(tgt_len, bsz, attn_enc_output.size(1))\n",
    "\n",
    "    # Here src is the original passed inputs to the 1st transformer encoder layer which \n",
    "    # are pe_src_embeds \n",
    "    # (src_len, bsz, embed_dim)\n",
    "    output_enc_1 = attn_enc_output + x\n",
    "\n",
    "    #  (src_len, bsz, embed_dim) @ (embed_dim) -> (src_len, bsz, embed_dim) \n",
    "    linear_result_enc_1 = output_enc_1*state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_1 = torch.nn.LayerNorm(normalized_shape=linear_result_enc_1.shape[2:])\n",
    "    linear_op_enc_1 = layernorm_enc_1(linear_result_enc_1)\n",
    "\n",
    "    # Manual Layer Normalization\n",
    "    x = linear_result_enc_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_1.weight\n",
    "    b = layernorm_enc_1.bias\n",
    "\n",
    "    linear_result_enc_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-05  \n",
    "    mean = linear_result_enc_1f.mean(dim=-1, keepdim=True)\n",
    "    # print(\"mean = \", mean)\n",
    "    std = linear_result_enc_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    # print(\"std =\", std)\n",
    "\n",
    "    # print((linear_result_enc_1f - mean) / (std + epsilon))\n",
    "\n",
    "    normalized_result_enc_1 = (linear_result_enc_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    # print(normalized_result_enc_1)\n",
    "\n",
    "    op_enc_1 = torch.matmul(normalized_result_enc_1, state_dict[\"transformer.encoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "\n",
    "    op_enc_1_relu = torch.nn.functional.relu(op_enc_1)\n",
    "\n",
    "    op_enc_2 = torch.matmul(op_enc_1_relu, state_dict[\"transformer.encoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "    \n",
    "\n",
    "    output_enc_2 = op_enc_2 + linear_op_enc_1\n",
    "    output_enc_2_norm = output_enc_2*state_dict[\"transformer.encoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_final = torch.nn.LayerNorm(normalized_shape=output_enc_2_norm.shape[2:])\n",
    "    output_enc_final = layernorm_enc_final(output_enc_2_norm)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization \n",
    "    x = output_enc_2_norm\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_final.weight\n",
    "    b = layernorm_enc_final.bias\n",
    "\n",
    "    linear_result_enc_2 = w*x + b\n",
    "\n",
    "    print(linear_result_enc_2)\n",
    "    epsilon = 1e-05  \n",
    "    mean = linear_result_enc_2.mean(dim=-1, keepdim=True)\n",
    "    # print(\"mean = \", mean)\n",
    "    std = linear_result_enc_2.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    # print(\"std = \", std)\n",
    "\n",
    "    # print((linear_result_enc_2 - mean))\n",
    "    output_enc_final = (linear_result_enc_2 - mean) / (std + epsilon) * w + b\n",
    "    \n",
    "\n",
    "    print(\"Final Encoder {} Output :\".format(layer_num))\n",
    "    print(\"norm2(norm1(x + self_atten(x)) + feed_fwd_op)\\n\")\n",
    "    print(output_enc_final)\n",
    "    print()\n",
    "\n",
    "    # (src_len, bsz, embed_dim) \n",
    "    return output_enc_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder functions to display the intermediate outputs and get the final outputs from the decoder\n",
    "\n",
    "\n",
    "### Self attention outputs from a decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_self_attn_output(x, state_dict, layer_num, num_heads, tgt_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec = key_dec = value_dec = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    " \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    Q_dec,K_dec,V_dec = get_qkv(query_dec, key_dec, value_dec ,W_dec, b_dec)\n",
    "    \n",
    "    # Q_dec = Q_dec.unsqueeze(0)\n",
    "    # K_dec = K_dec.unsqueeze(0)\n",
    "    # V_dec = V_dec.unsqueeze(0)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim)\n",
    "    # print(Q_dec.shape, K_dec.shape , V_dec.shape)\n",
    "    # print(tgt_len, bsz * num_heads, head_dim)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim) -> ( bsz*num_heads, tgt_len , head_dim )\n",
    "    Q_dec = Q_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec = K_dec.reshape(K_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec = V_dec.reshape(V_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec)\n",
    "    print()\n",
    "\n",
    "    src_len = K_dec.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = tgt_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "    \n",
    "    \n",
    "    if need_weights is False:\n",
    "        attn_output = atten_product_needs_wts_false(Q = Q_dec, V = V_dec, K = K_dec, bsz = bsz, head_dim=head_dim, src_len=src_len, tgt_len=tgt_len, attn_mask = attn_mask, embed_dim=embed_dim, num_heads=num_heads\n",
    "        )\n",
    "\n",
    "        print(\"Decoder Self Attention = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_dec_output, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_dec_output,attn_wt_matrix_dec = atten_product_needs_wts_true(Q=Q_dec, K=K_dec, V=V_dec, bsz=bsz, tgt_len=tgt_len, attn_mask = attn_mask, embed_dim=embed_dim)\n",
    "\n",
    "        print(\"Decoder Attention output = \")\n",
    "        print(attn_wt_matrix_dec)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "    \n",
    "\n",
    "        return attn_dec_output, attn_wt_matrix_dec\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to perform the linear layer calculations after decoder's self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_post_self_attn(self_attn_dec, x, state_dict, layer_num):\n",
    "\n",
    "    # (bsz*tgt_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*tgt_len , embed_dim)\n",
    "    output_dec_1 = self_attn_dec + x\n",
    "\n",
    "    print(output_dec_1)\n",
    "\n",
    "\n",
    "    # (bsz*tgt_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*tgt_len , embed_dim)\n",
    "    linear_result_dec_1 = output_dec_1*state_dict[\"transformer.decoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n",
    "    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n",
    "\n",
    "    # From torch's implementation\n",
    "    # print(linear_op_dec_1)\n",
    "\n",
    "    x = linear_result_dec_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_1.weight\n",
    "    b = layernorm_dec_1.bias\n",
    "\n",
    "    linear_result_dec_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-05  \n",
    "    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    print(\"Decoder_{} norm1(x + sa(x))\".format(layer_num))\n",
    "    print(normalized_result_dec_1)\n",
    "    print()\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    return normalized_result_dec_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross attention in decoder \n",
    "### query, key, value =  x, mem, mem \n",
    "\n",
    "### Cross attention between the encoder's final output and the decoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num, tgt_len, src_len, head_dim, num_heads, memory_mask = None,need_weights = False):\n",
    "\n",
    "    print(\"MEMORY = \",memory)\n",
    "    print()\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec_mha = x_dec\n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    key_dec_mha, value_dec_mha = memory, memory\n",
    "\n",
    "    tgt_len, bsz, embed_dim = query_dec_mha.shape\n",
    "\n",
    "    W_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    ########## CHANGE FROM SELF ATTENTION ############\n",
    "\n",
    "    Q_dec_mha,K_dec_mha,V_dec_mha = get_qkv(query_dec_mha, key_dec_mha, value_dec_mha ,W_dec_mha, b_dec_mha)\n",
    "\n",
    "\n",
    "    #################################################\n",
    "\n",
    "    # K_dec_mha = K_dec_mha.unsqueeze(0)\n",
    "    # V_dec_mha = V_dec_mha.unsqueeze(0)\n",
    "    # Q_dec_mha = Q_dec_mha.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_dec_mha = Q_dec_mha.reshape(Q_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec_mha = K_dec_mha.reshape(K_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec_mha = V_dec_mha.reshape(V_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec_mha)\n",
    "    print()\n",
    "\n",
    "\n",
    "    attn_mask = memory_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output_dec_mha = atten_product_needs_wts_false(Q =Q_dec_mha, V=V_dec_mha, K=K_dec_mha, bsz=bsz, head_dim=head_dim, src_len=src_len, tgt_len=tgt_len, attn_mask=attn_mask, embed_dim=embed_dim, num_heads=num_heads)\n",
    "        \n",
    "        print(\"Cross attention in decoder_{}\".format(layer_num))\n",
    "        print(attn_output_dec_mha)\n",
    "        print()\n",
    "\n",
    "        op_dec_mha_1 = torch.matmul(attn_output_dec_mha, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "        attn_dec_mha_output = op_dec_mha_1.view(tgt_len, bsz, attn_output_dec_mha.size(1))\n",
    "\n",
    "        return attn_dec_mha_output, None\n",
    "    \n",
    "    else: \n",
    "\n",
    "    \n",
    "        attn_dec_mha_output ,attn_wt_matrix_dec_mha = atten_product_needs_wts_true(Q=Q_dec_mha, K=K_dec_mha, V=V_dec_mha, bsz=bsz, tgt_len=tgt_len, attn_mask=attn_mask, embed_dim=embed_dim)\n",
    "\n",
    "        print(\"Decoder mha Attention output = \")\n",
    "        print(attn_dec_mha_output)\n",
    "        print()\n",
    "\n",
    "\n",
    "        op_dec_mha = torch.matmul(attn_dec_mha_output, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output_mha = op_dec_mha.view(tgt_len, bsz, attn_dec_mha_output.size(1))\n",
    "\n",
    "\n",
    "        return attn_dec_output_mha , attn_wt_matrix_dec_mha\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to perform the linear layer calculations after Decoder's cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    output_dec_2 = attn_dec_mha_output + x_dec\n",
    "\n",
    "    print(output_dec_2)\n",
    "\n",
    "    linear_result_dec_2 = output_dec_2*state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_2 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_2.shape[2:])\n",
    "    linear_op_dec_2 = layernorm_dec_2(linear_result_dec_2)\n",
    "\n",
    "    x = linear_result_dec_2\n",
    "    w = layernorm_dec_2.weight\n",
    "    b = layernorm_dec_2.bias\n",
    "\n",
    "    linear_result_dec_2f = w*x + b\n",
    "\n",
    "    epsilon = 1e-05  \n",
    "    mean = linear_result_dec_2f.mean(dim=-1, keepdim=True)\n",
    "    # print(\"mean = \", mean)\n",
    "    std = linear_result_dec_2f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    # print(\"std = \", std)\n",
    "    normalized_result_dec_2 = (linear_result_dec_2f - mean) / (std + epsilon) * w + b\n",
    "    print((linear_result_dec_2f - mean))\n",
    "    print(\"norm2(x' + mha(x', mem)), \\n where x' = Decoder_curr_layer norm1(x + sa(x))\")\n",
    "    print(normalized_result_dec_2)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    x_dec2_norm = normalized_result_dec_2\n",
    "\n",
    "    op_dec_1 = torch.matmul(x_dec2_norm, state_dict[\"transformer.decoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "\n",
    "    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n",
    "    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"transformer.decoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    ff_dec = op_dec_2\n",
    "\n",
    "    x_dec3_unorm = x_dec2_norm + ff_dec\n",
    "\n",
    "    linear_result_dec_3 = x_dec3_unorm*state_dict[\"transformer.decoder.layers.0.norm3.weight\"] + state_dict[\"transformer.decoder.layers.0.norm3.bias\"]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_3 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_3.shape[2:])\n",
    "    linear_op_dec_3 = layernorm_dec_3(linear_result_dec_3)\n",
    "\n",
    "    # From torch's implementation\n",
    "    x = linear_result_dec_3\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_3.weight\n",
    "    b = layernorm_dec_3.bias\n",
    "\n",
    "    linear_result_dec_3f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_3f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_3f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    print(\"mean = \", mean)\n",
    "    print(\"std = \", std)\n",
    "\n",
    "    print( (linear_result_dec_3f - mean))\n",
    "    normalized_result_dec_3 = (linear_result_dec_3f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm3(x'' + ff(x'')) \\n where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\")\n",
    "    print(normalized_result_dec_3)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_3\n",
    "\n",
    "def feef_fwd_transformer(dec_output_final, state_dict):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim) @ (vocab_size, embed_dim).T -> (tgt_len, bsz, vocab_size)\n",
    "    final_op = dec_output_final@state_dict[\"fc.weight\"].T + state_dict[\"fc.bias\"]\n",
    "\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the same transformer as in the other notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "\n",
    "\n",
    "class TransformerModel1(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff, dropout = 0):\n",
    "\n",
    "        super(TransformerModel1, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout,\n",
    "            dim_feedforward=d_ff,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "\n",
    "        src_mask = None\n",
    "        seq_length = tgt.size(0)\n",
    "        \n",
    "        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "        return src_mask, nopeak_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "\n",
    "        print(\"Tgt mask shape = \", tgt_mask.shape)\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "\n",
    "        output = self.transformer(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask, tgt_is_causal = False)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:293: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "src_vocab_size = 20\n",
    "tgt_vocab_size = 20\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "d_ff = 20\n",
    "max_seq_len = 5\n",
    "dropout = 0\n",
    "\n",
    "# src_vocab_size = 20\n",
    "# tgt_vocab_size = 20\n",
    "# d_model = 2\n",
    "# num_heads = 1\n",
    "# num_encoder_layers = 1\n",
    "# num_decoder_layers = 1\n",
    "# d_ff = 3\n",
    "# max_seq_len = 4\n",
    "# dropout = 0\n",
    "\n",
    "\n",
    "transformer = TransformerModel1(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff)\n",
    "\n",
    "# Generate random sample data\n",
    "# src_data = torch.randint(1, src_vocab_size, (max_seq_len , 3))  # (seq_length, batch_size,)\n",
    "# tgt_data = torch.randint(1, tgt_vocab_size, ( max_seq_len, 3))  # (seq_length, batch_size)\n",
    "\n",
    "# src_data = torch.tensor([[0, 2, 4], [1, 0, 7], [2, 2, 0], [3, 5, 6], [6, 1, 9]])\n",
    "# tgt_data = torch.tensor([[1, 7, 9], [3, 4, 1], [5, 2, 8], [8, 0, 3], [4, 5, 9]]) \n",
    "\n",
    "src_data = torch.tensor([[2], [1], [5], [4]])\n",
    "tgt_data = torch.tensor([[1], [16], [5], [3], [9]]) \n",
    "\n",
    "\n",
    "state_dict = transformer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=512, dropout=0):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.encoding = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "#         self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "#         self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "#         self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.encoding[:, :x.size(1)].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "state_dict1 = copy.deepcopy(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1]), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data.shape, tgt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_data.view(-1)\n",
    "tgt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1]), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data.shape, tgt_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross verifying the intermediate outputs for the 1st forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(src, tgt):\n",
    "    src_mask = None\n",
    "    seq_length = tgt.size(0)\n",
    "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "    # tgt_mask = tgt_mask & nopeak_mask\n",
    "    return src_mask, nopeak_mask\n",
    "\n",
    "src_mask, tgt_mask = generate_mask(src_data, tgt_data[:-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False, False]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_intermediate_outputs_mask(src_sentence, tgt_sentence,d_model, num_heads ,state_dict, num_encoder_layers , num_decoder_layers, tgt_mask, d_ff):\n",
    "\n",
    "    pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence=src_sentence,  tgt_sentence=tgt_sentence, state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n",
    "    \n",
    "    print(\"###\"*25)\n",
    "    print(\"### Encoder Start ###\")\n",
    "    print()\n",
    "\n",
    "    x_enc = pe_src_embeds\n",
    "\n",
    "    for lno in range(num_encoder_layers):\n",
    "        attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(x_enc, state_dict, layer_num = lno, need_weights = False, embed_dim = d_model, num_heads= num_heads, src_mask=None)\n",
    "\n",
    "        if lno == 0:\n",
    "            tgt_len, bsz, embed_dim = x_enc.shape\n",
    "\n",
    "        output_enc_final = encoder_block_post_attn_output(x_enc, attn_enc_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n",
    "\n",
    "        x_enc = output_enc_final\n",
    "\n",
    "    \n",
    "    print(\"### Encoder Done ###\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"### Decoder Start ###\")\n",
    "\n",
    "    x_dec = pe_tgt_embeds\n",
    "    memory = x_enc\n",
    "\n",
    "    for lno in range(num_decoder_layers):\n",
    "\n",
    "        self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, need_weights = False, tgt_mask=tgt_mask, num_heads = num_heads)\n",
    "        x_dec = dec_post_self_attn(self_attn_dec, x_dec, state_dict, layer_num = lno)\n",
    "\n",
    "        attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, num_heads = num_heads, layer_num = lno, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = False, memory_mask=None)\n",
    "        final_op = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = lno)\n",
    "\n",
    "        print(pe_tgt_embeds.shape, final_op.shape)\n",
    "        x_dec = final_op\n",
    "\n",
    "    print(\"### Decoder Done ###\")\n",
    "\n",
    "    final_op = feef_fwd_transformer(final_op, state_dict)\n",
    "\n",
    "    return final_op\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict1 = transformer.state_dict\n",
    "\n",
    "src_mask, tgt_mask = generate_mask(src_data, tgt_data[:-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False, False]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2],\n",
       "         [1],\n",
       "         [5],\n",
       "         [4]]),\n",
       " tensor([[ 1],\n",
       "         [16],\n",
       "         [ 5],\n",
       "         [ 3]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data, tgt_data[:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 2, Embedding: tensor([-0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,  0.6408,  0.4412,\n",
      "        -0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867])\n",
      "Word index: 1, Embedding: tensor([-1.3527, -1.6959,  0.5667,  0.7935,  0.5988, -1.5551, -0.3414,  1.8530,\n",
      "         0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, -0.8437])\n",
      "Word index: 5, Embedding: tensor([-9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n",
      "        -4.0003e-01,  1.0395e+00,  3.5815e-01, -2.4600e-01,  2.3025e+00,\n",
      "        -1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01,  3.3532e-02,\n",
      "         7.1009e-01])\n",
      "Word index: 4, Embedding: tensor([-0.5692,  0.9200,  1.1108,  1.2899, -1.4782,  2.5672, -0.4731,  0.3356,\n",
      "        -1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149, -0.1407,  0.8058])\n",
      "\n",
      "torch.Size([4, 1, 16])\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([ 0.6442,  3.9300, -0.1244,  0.2953,  0.3827, -0.5497, -0.9940,  1.3459,\n",
      "         1.9457, -1.2904, -2.3495, -2.0689,  0.9094, -0.6946,  1.9595, -1.1038])\n",
      "Word index: 16, Embedding: tensor([-0.8733,  0.0043, -1.2579, -1.0845,  0.7530,  0.3236, -0.2750,  1.3056,\n",
      "         0.2118,  0.2720, -0.9268, -2.7330, -0.5642, -0.2740,  0.1398,  0.5086])\n",
      "Word index: 5, Embedding: tensor([-0.7645,  0.2408,  0.1664, -2.2318,  1.3892, -0.5023,  1.6797, -1.0240,\n",
      "         1.6859, -1.2177,  0.7650,  1.1971, -0.7128, -0.0656,  2.2050,  1.7852])\n",
      "Word index: 3, Embedding: tensor([ 0.4990,  0.8780,  0.3894,  1.4625,  0.4795, -0.5334, -0.0347,  0.6573,\n",
      "        -0.3112, -0.5620, -0.4835, -1.2721, -0.1740,  0.5541, -0.1817, -0.2345])\n",
      "\n",
      "PE of src :\n",
      "torch.Size([4, 1])\n",
      "tensor([[[0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.]]])\n",
      "\n",
      "PE of tgt :\n",
      "torch.Size([4, 1])\n",
      "tensor([[[0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.]]])\n",
      "\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-0.6136,  1.0316, -0.4927,  1.2484,  0.4397,  1.1124,  0.6408,\n",
      "           1.4412, -0.1023,  1.7924, -0.2897,  1.0525,  0.5229,  3.3022,\n",
      "          -1.4689, -0.5867]],\n",
      "\n",
      "        [[-1.3527, -0.6959,  0.5667,  1.7935,  0.5988, -0.5551, -0.3414,\n",
      "           2.8530,  0.7502,  0.4145, -0.1734,  1.1835,  1.3894,  2.5863,\n",
      "           0.9463,  0.1563]],\n",
      "\n",
      "        [[-0.0933,  1.6871, -0.8383,  1.0009,  0.8419,  0.6000,  1.0395,\n",
      "           1.3582, -0.2460,  3.3025, -1.8817,  0.9503, -1.0450,  0.0435,\n",
      "           0.0335,  1.7101]],\n",
      "\n",
      "        [[-0.5692,  1.9200,  1.1108,  2.2899, -1.4782,  3.5672, -0.4731,\n",
      "           1.3356, -1.6293,  0.4503, -0.4798,  0.5003, -1.0670,  2.1149,\n",
      "          -0.1407,  1.8058]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[ 0.6442,  4.9300, -0.1244,  1.2953,  0.3827,  0.4503, -0.9940,\n",
      "           2.3459,  1.9457, -0.2904, -2.3495, -1.0689,  0.9094,  0.3054,\n",
      "           1.9595, -0.1038]],\n",
      "\n",
      "        [[-0.8733,  1.0043, -1.2579, -0.0845,  0.7530,  1.3236, -0.2750,\n",
      "           2.3056,  0.2118,  1.2720, -0.9268, -1.7330, -0.5642,  0.7260,\n",
      "           0.1398,  1.5086]],\n",
      "\n",
      "        [[-0.7645,  1.2408,  0.1664, -1.2318,  1.3892,  0.4977,  1.6797,\n",
      "          -0.0240,  1.6859, -0.2177,  0.7650,  2.1971, -0.7128,  0.9344,\n",
      "           2.2050,  2.7852]],\n",
      "\n",
      "        [[ 0.4990,  1.8780,  0.3894,  2.4625,  0.4795,  0.4666, -0.0347,\n",
      "           1.6573, -0.3112,  0.4380, -0.4835, -0.2721, -0.1740,  1.5541,\n",
      "          -0.1817,  0.7655]]])\n",
      "\n",
      "###########################################################################\n",
      "### Encoder Start ###\n",
      "\n",
      "Q_enc_shape =  torch.Size([4, 1, 16])\n",
      "K_enc_shape =  torch.Size([4, 1, 16])\n",
      "V_enc_shape =  torch.Size([4, 1, 16])\n",
      "\n",
      "Q_enc_shape =  torch.Size([4, 4, 4])\n",
      "K_enc_shape =  torch.Size([4, 4, 4])\n",
      "V_enc_shape =  torch.Size([4, 4, 4])\n",
      "\n",
      "Q_enc_0 = \n",
      "tensor([[[-1.3674,  0.3845, -0.6208,  0.3121],\n",
      "         [-1.1174,  0.7566, -0.9770,  1.4006],\n",
      "         [-1.3841,  0.4940, -0.6182, -1.5365],\n",
      "         [-0.8299, -0.5594, -1.7956, -0.3519]],\n",
      "\n",
      "        [[ 1.4387, -0.0212,  0.6754, -0.1537],\n",
      "         [ 1.0749,  0.9371, -0.4511,  0.6815],\n",
      "         [-0.0743, -0.5228,  0.7193, -1.1888],\n",
      "         [ 0.9846, -0.7004,  1.1799, -0.5392]],\n",
      "\n",
      "        [[-0.4807,  1.1659,  1.0318,  0.8863],\n",
      "         [-0.2968,  1.2599,  0.7989,  0.9074],\n",
      "         [-0.1183,  0.5442,  2.0783, -0.0790],\n",
      "         [-0.9945, -0.4080,  0.7382,  1.6337]],\n",
      "\n",
      "        [[ 0.1829, -0.6673, -1.2892,  1.0165],\n",
      "         [-0.4374, -0.4559, -2.2021,  1.0927],\n",
      "         [ 0.2552,  0.3247,  1.0544, -1.1998],\n",
      "         [ 1.9266,  0.5328,  0.6005,  0.6618]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[ 1.3886, -0.0958, -0.1802,  1.1143],\n",
      "         [ 0.8551, -0.3208,  0.1280,  1.4454],\n",
      "         [ 1.4976, -1.3598, -0.6594, -0.1428],\n",
      "         [ 1.3388,  0.8645, -0.1417,  0.0854]],\n",
      "\n",
      "        [[-2.2233, -0.4085,  0.5634, -1.0690],\n",
      "         [-0.4495, -0.4241,  0.4524, -0.0219],\n",
      "         [-1.4950, -0.8725, -0.2811,  0.2547],\n",
      "         [-0.7670, -0.5551, -0.8928, -1.3067]],\n",
      "\n",
      "        [[-0.2002, -0.1338,  0.8681, -0.5048],\n",
      "         [-1.4552,  0.5602,  0.1951, -0.2186],\n",
      "         [ 1.2765, -0.1590,  1.2154, -0.8634],\n",
      "         [ 2.0171, -1.2824,  1.5906, -0.3614]],\n",
      "\n",
      "        [[ 0.9071,  0.3881, -0.2993, -0.3884],\n",
      "         [ 0.4064,  0.8863, -0.1283, -1.3372],\n",
      "         [ 0.7261,  0.6361, -0.0344,  0.0973],\n",
      "         [-0.4079,  0.1462,  1.9521, -1.4231]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[ 5.3863e-01,  3.8400e-01,  2.6563e-01,  4.8029e-01],\n",
      "         [ 9.0668e-01,  1.1015e-01, -5.3271e-01, -2.9298e-01],\n",
      "         [ 3.0137e-01, -1.1610e+00, -4.9984e-01,  5.2922e-01],\n",
      "         [ 9.3528e-01,  1.7174e+00,  1.5208e+00, -8.1570e-01]],\n",
      "\n",
      "        [[-2.6104e-02,  3.4447e-01, -9.4538e-01,  1.0343e+00],\n",
      "         [ 1.7192e-01,  2.9887e-01, -1.0815e+00, -6.9819e-02],\n",
      "         [ 1.0957e+00,  3.7487e-01,  2.2780e-01,  1.8395e-01],\n",
      "         [ 1.8900e+00, -4.9312e-01, -1.4716e-01,  1.0706e+00]],\n",
      "\n",
      "        [[-3.9131e-01, -1.5088e+00, -2.4390e-01,  8.5713e-01],\n",
      "         [-5.9327e-01, -1.0738e+00,  1.5256e-01,  9.9306e-02],\n",
      "         [ 6.3308e-01, -1.3190e+00,  1.1270e+00, -1.2171e-01],\n",
      "         [ 4.9501e-01,  1.0747e+00,  4.2425e-02,  7.7182e-01]],\n",
      "\n",
      "        [[ 3.6931e-01,  4.5480e-01,  1.0756e+00, -4.0149e-01],\n",
      "         [-6.2147e-01,  1.3152e+00,  4.9513e-01, -2.2198e+00],\n",
      "         [ 8.5603e-01,  1.4122e+00,  7.0563e-01,  1.1181e+00],\n",
      "         [ 1.7480e+00,  5.9352e-01,  2.0649e+00, -7.7772e-04]]])\n",
      "\n",
      "Attn weight =  tensor([[[[0.2462, 0.3250, 0.1709, 0.2579],\n",
      "          [0.2982, 0.4002, 0.0911, 0.2105],\n",
      "          [0.1451, 0.1400, 0.3000, 0.4148],\n",
      "          [0.1830, 0.1739, 0.4778, 0.1653]],\n",
      "\n",
      "         [[0.1406, 0.4475, 0.1620, 0.2499],\n",
      "          [0.1036, 0.3910, 0.2343, 0.2711],\n",
      "          [0.3960, 0.1919, 0.1462, 0.2659],\n",
      "          [0.2278, 0.3873, 0.1631, 0.2218]],\n",
      "\n",
      "         [[0.2635, 0.4283, 0.1858, 0.1225],\n",
      "          [0.2569, 0.4171, 0.1983, 0.1277],\n",
      "          [0.2317, 0.1481, 0.3069, 0.3133],\n",
      "          [0.2761, 0.4408, 0.1130, 0.1701]],\n",
      "\n",
      "         [[0.3910, 0.1749, 0.3820, 0.0520],\n",
      "          [0.3974, 0.1953, 0.3806, 0.0267],\n",
      "          [0.1137, 0.2236, 0.0994, 0.5634],\n",
      "          [0.3277, 0.1776, 0.3739, 0.1208]]]])\n",
      "\n",
      "tensor([[[[0.2462, 0.3250, 0.1709, 0.2579],\n",
      "          [0.2982, 0.4002, 0.0911, 0.2105],\n",
      "          [0.1451, 0.1400, 0.3000, 0.4148],\n",
      "          [0.1830, 0.1739, 0.4778, 0.1653]],\n",
      "\n",
      "         [[0.1406, 0.4475, 0.1620, 0.2499],\n",
      "          [0.1036, 0.3910, 0.2343, 0.2711],\n",
      "          [0.3960, 0.1919, 0.1462, 0.2659],\n",
      "          [0.2278, 0.3873, 0.1631, 0.2218]],\n",
      "\n",
      "         [[0.2635, 0.4283, 0.1858, 0.1225],\n",
      "          [0.2569, 0.4171, 0.1983, 0.1277],\n",
      "          [0.2317, 0.1481, 0.3069, 0.3133],\n",
      "          [0.2761, 0.4408, 0.1130, 0.1701]],\n",
      "\n",
      "         [[0.3910, 0.1749, 0.3820, 0.0520],\n",
      "          [0.3974, 0.1953, 0.3806, 0.0267],\n",
      "          [0.1137, 0.2236, 0.0994, 0.5634],\n",
      "          [0.3277, 0.1776, 0.3739, 0.1208]]]])\n",
      "ATT =  tensor([[[[ 0.7200,  0.3747,  0.1990, -0.0968],\n",
      "          [ 0.7478,  0.4142,  0.1406, -0.0975],\n",
      "          [ 0.6835,  0.4352,  0.4449, -0.1509],\n",
      "          [ 0.5549, -0.1814, -0.0314,  0.1549]],\n",
      "\n",
      "         [[ 0.7231,  0.1197, -0.6168,  0.4115],\n",
      "          [ 0.8335,  0.1067, -0.5074,  0.4132],\n",
      "          [ 0.6854,  0.1175, -0.5877,  0.7077],\n",
      "          [ 0.6585,  0.1460, -0.6297,  0.4760]],\n",
      "\n",
      "         [[-0.1790, -0.9709,  0.2156,  0.3403],\n",
      "          [-0.1592, -0.9598,  0.2299,  0.3361],\n",
      "          [ 0.1708, -0.5768,  0.3252,  0.4178],\n",
      "          [-0.2139, -0.8562,  0.1344,  0.3980]],\n",
      "\n",
      "         [[ 0.4537,  0.9783,  0.8842, -0.1181],\n",
      "          [ 0.3979,  0.9909,  0.8478, -0.1675],\n",
      "          [ 0.9729,  0.8204,  1.4665, -0.4312],\n",
      "          [ 0.5419,  0.9824,  0.9537, -0.1079]]]])\n",
      "Attention output = \n",
      "torch.Size([1, 4, 4, 4]) tensor([[[[0.2462, 0.3250, 0.1709, 0.2579],\n",
      "          [0.2982, 0.4002, 0.0911, 0.2105],\n",
      "          [0.1451, 0.1400, 0.3000, 0.4148],\n",
      "          [0.1830, 0.1739, 0.4778, 0.1653]],\n",
      "\n",
      "         [[0.1406, 0.4475, 0.1620, 0.2499],\n",
      "          [0.1036, 0.3910, 0.2343, 0.2711],\n",
      "          [0.3960, 0.1919, 0.1462, 0.2659],\n",
      "          [0.2278, 0.3873, 0.1631, 0.2218]],\n",
      "\n",
      "         [[0.2635, 0.4283, 0.1858, 0.1225],\n",
      "          [0.2569, 0.4171, 0.1983, 0.1277],\n",
      "          [0.2317, 0.1481, 0.3069, 0.3133],\n",
      "          [0.2761, 0.4408, 0.1130, 0.1701]],\n",
      "\n",
      "         [[0.3910, 0.1749, 0.3820, 0.0520],\n",
      "          [0.3974, 0.1953, 0.3806, 0.0267],\n",
      "          [0.1137, 0.2236, 0.0994, 0.5634],\n",
      "          [0.3277, 0.1776, 0.3739, 0.1208]]]])\n",
      "tensor([[[ 3.1887e-01, -7.8532e-01, -5.6266e-01,  5.0947e-01,  2.5817e-03,\n",
      "           6.7278e-01, -1.3902e+00,  2.1134e+00, -4.8928e-01,  2.1886e+00,\n",
      "           7.2336e-01,  5.4132e-01, -1.0290e+00,  2.5880e+00, -1.9003e+00,\n",
      "           6.0905e-02]],\n",
      "\n",
      "        [[-7.8241e-01, -2.4806e+00, -3.6759e-01,  1.9097e+00, -9.2756e-02,\n",
      "           8.5840e-02, -3.1238e+00,  2.5732e+00,  2.0708e-01,  9.3287e-01,\n",
      "           1.3228e-01,  9.2788e-01, -1.4724e-01,  2.6474e+00, -8.4863e-01,\n",
      "          -3.9076e-01]],\n",
      "\n",
      "        [[ 4.2668e-01, -4.4286e-01, -1.0387e+00,  6.6544e-02,  1.5230e+00,\n",
      "          -1.2224e-01, -2.4665e-01,  1.9021e+00, -1.4647e+00,  3.2163e+00,\n",
      "          -4.1617e-01,  1.3722e+00, -1.1310e+00, -9.6696e-01,  1.5659e-01,\n",
      "           1.6839e+00]],\n",
      "\n",
      "        [[ 5.9466e-01,  3.3022e-01,  1.2539e+00,  1.6022e+00, -2.2865e+00,\n",
      "           1.6812e+00, -2.9575e+00,  1.0351e+00, -9.4813e-01,  2.2418e+00,\n",
      "           4.6480e-01, -3.9806e-01, -2.1403e+00,  2.4785e+00, -6.8668e-01,\n",
      "           2.4221e+00]]], grad_fn=<AddBackward0>)\n",
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[ 0.0776, -0.8134, -0.6337,  0.2314, -0.1776,  0.3632, -1.3015,\n",
      "           1.5257, -0.5745,  1.5865,  0.4040,  0.2572, -1.0101,  1.9087,\n",
      "          -1.7131, -0.1305]],\n",
      "\n",
      "        [[-0.5666, -1.6901, -0.2921,  1.2146, -0.1103,  0.0079, -2.1157,\n",
      "           1.6536,  0.0881,  0.5683,  0.0386,  0.5650, -0.1463,  1.7027,\n",
      "          -0.6104, -0.3074]],\n",
      "\n",
      "        [[ 0.1136, -0.5709, -1.0399, -0.1699,  0.9766, -0.3185, -0.4165,\n",
      "           1.2750, -1.3753,  2.3096, -0.5499,  0.8579, -1.1126, -0.9835,\n",
      "          -0.0990,  1.1033]],\n",
      "\n",
      "        [[ 0.1810,  0.0224,  0.5764,  0.7853, -1.5473,  0.8328, -1.9498,\n",
      "           0.4452, -0.7445,  1.1691,  0.1031, -0.4145, -1.4597,  1.3111,\n",
      "          -0.5877,  1.2772]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "### Encoder Done ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Decoder Start ###\n",
      "Q_dec_0 = \n",
      "tensor([[[-0.4777,  0.6999, -0.2432,  0.1375],\n",
      "         [ 0.8145, -0.4431,  0.5472, -0.1131],\n",
      "         [-0.9603,  2.4673, -0.2960,  2.9330],\n",
      "         [-0.2045,  0.6661,  0.3576, -0.3115]],\n",
      "\n",
      "        [[-0.9855, -0.7563,  2.1984, -1.0317],\n",
      "         [ 0.2467,  0.8896, -0.3073, -0.9787],\n",
      "         [-1.8930,  0.5848,  1.4320,  0.1252],\n",
      "         [-1.5500,  0.3509, -0.0688, -0.5219]],\n",
      "\n",
      "        [[-0.8340,  1.4440,  0.4027, -0.4441],\n",
      "         [ 0.4354, -0.3233,  0.6646,  0.1838],\n",
      "         [-0.3863, -1.6106, -0.6367,  0.1907],\n",
      "         [-0.5045,  0.3989, -0.0272, -0.2692]],\n",
      "\n",
      "        [[ 0.5070,  1.3615, -0.8332, -0.1513],\n",
      "         [-0.1089,  0.7277, -0.0139,  0.4557],\n",
      "         [ 2.7266,  1.4596,  1.5861,  1.1367],\n",
      "         [ 0.2242,  1.6721, -1.2046,  0.0387]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 7.8398e-01, -2.8978e+00, -1.9969e+00, -8.1523e-02],\n",
      "         [ 1.1943e-01, -1.3287e+00,  6.8752e-02,  7.7737e-01],\n",
      "         [ 2.2459e+00,  1.1807e-01, -8.4272e-01, -5.2088e-01],\n",
      "         [-2.4054e-01, -1.1789e+00, -3.2179e-01,  3.0028e-01]],\n",
      "\n",
      "        [[-2.2458e-01, -1.4127e-01, -1.5425e-02, -4.3561e-01],\n",
      "         [ 1.0355e-01, -1.6613e+00,  5.0868e-01, -8.0030e-01],\n",
      "         [-8.1991e-01,  9.2554e-01,  3.2352e-02,  7.1836e-01],\n",
      "         [-4.5286e-01, -1.7143e-01,  7.3920e-01, -2.2124e-01]],\n",
      "\n",
      "        [[ 4.3926e-01,  1.0542e+00,  2.5666e-03, -2.5535e+00],\n",
      "         [-2.8253e-01,  1.1993e+00, -4.7817e-01, -6.8918e-01],\n",
      "         [ 5.9615e-01, -2.5148e-01,  2.7867e-01, -1.7343e+00],\n",
      "         [ 8.3438e-01,  4.8703e-01,  3.7392e-01, -3.1019e-01]],\n",
      "\n",
      "        [[ 2.1036e-01,  1.3534e+00, -4.9534e-01, -2.2155e+00],\n",
      "         [ 8.7180e-01,  7.8376e-01, -1.0351e+00, -1.7170e-01],\n",
      "         [ 8.3970e-01,  8.2966e-01,  1.5627e-01, -1.7664e-01],\n",
      "         [-2.2480e-01,  1.1705e+00, -5.1477e-01, -4.6465e-01]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-1.0260,  0.9662, -2.2913,  1.8057],\n",
      "         [ 0.0932, -0.9930,  0.0467,  0.6059],\n",
      "         [ 1.1529,  0.6390,  0.4691,  0.1807],\n",
      "         [ 0.5957, -0.4363, -0.6107,  0.1757]],\n",
      "\n",
      "        [[-0.0240, -1.3353, -1.6751, -0.9049],\n",
      "         [-0.1854, -0.8408, -0.1099,  0.0215],\n",
      "         [ 0.2908,  0.4321,  1.0739, -0.3611],\n",
      "         [-0.4670, -1.8406, -0.9363, -0.2806]],\n",
      "\n",
      "        [[ 1.5304, -1.8514, -1.5147,  0.9213],\n",
      "         [ 1.1991, -1.0642, -2.1201,  0.6408],\n",
      "         [ 1.3649, -0.4078, -1.0577, -1.9731],\n",
      "         [ 0.9723, -0.8812, -0.8274,  1.0724]],\n",
      "\n",
      "        [[ 0.5280,  0.7117, -0.2574,  1.5133],\n",
      "         [ 0.3326,  0.7507,  0.3085, -0.9582],\n",
      "         [ 0.0418, -0.3742,  0.1779,  0.3042],\n",
      "         [ 0.4975,  0.9409, -0.3728, -0.8591]]])\n",
      "\n",
      "Attnetion mask infunction = \n",
      "tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "\n",
      "Attn weight =  tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5254, 0.4746, 0.0000, 0.0000],\n",
      "          [0.0287, 0.7110, 0.2602, 0.0000],\n",
      "          [0.1126, 0.2572, 0.3485, 0.2818]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6313, 0.3687, 0.0000, 0.0000],\n",
      "          [0.2305, 0.1541, 0.6154, 0.0000],\n",
      "          [0.2410, 0.1546, 0.3406, 0.2637]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5422, 0.4578, 0.0000, 0.0000],\n",
      "          [0.1933, 0.2752, 0.5315, 0.0000],\n",
      "          [0.3103, 0.3001, 0.2051, 0.1845]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4437, 0.5563, 0.0000, 0.0000],\n",
      "          [0.0770, 0.2608, 0.6622, 0.0000],\n",
      "          [0.3026, 0.2914, 0.1472, 0.2588]]]])\n",
      "\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5254, 0.4746, 0.0000, 0.0000],\n",
      "          [0.0287, 0.7110, 0.2602, 0.0000],\n",
      "          [0.1126, 0.2572, 0.3485, 0.2818]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6313, 0.3687, 0.0000, 0.0000],\n",
      "          [0.2305, 0.1541, 0.6154, 0.0000],\n",
      "          [0.2410, 0.1546, 0.3406, 0.2637]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5422, 0.4578, 0.0000, 0.0000],\n",
      "          [0.1933, 0.2752, 0.5315, 0.0000],\n",
      "          [0.3103, 0.3001, 0.2051, 0.1845]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4437, 0.5563, 0.0000, 0.0000],\n",
      "          [0.0770, 0.2608, 0.6622, 0.0000],\n",
      "          [0.3026, 0.2914, 0.1472, 0.2588]]]])\n",
      "ATT =  tensor([[[[-1.0260e+00,  9.6616e-01, -2.2913e+00,  1.8057e+00],\n",
      "          [-4.9481e-01,  3.6326e-02, -1.1817e+00,  1.2362e+00],\n",
      "          [ 3.3676e-01, -5.1201e-01,  8.9419e-02,  5.2970e-01],\n",
      "          [ 4.7802e-01, -4.6889e-02, -2.5458e-01,  4.7160e-01]],\n",
      "\n",
      "         [[-2.3983e-02, -1.3353e+00, -1.6751e+00, -9.0489e-01],\n",
      "          [-8.3502e-02, -1.1530e+00, -1.0980e+00, -5.6330e-01],\n",
      "          [ 1.4487e-01, -1.7150e-01,  2.5771e-01, -4.2746e-01],\n",
      "          [-5.8512e-02, -7.9001e-01, -3.0185e-01, -4.1176e-01]],\n",
      "\n",
      "         [[ 1.5304e+00, -1.8514e+00, -1.5147e+00,  9.2130e-01],\n",
      "          [ 1.3787e+00, -1.4910e+00, -1.7919e+00,  7.9285e-01],\n",
      "          [ 1.3513e+00, -8.6753e-01, -1.4385e+00, -6.9414e-01],\n",
      "          [ 1.2941e+00, -1.1401e+00, -1.4758e+00,  2.7135e-01]],\n",
      "\n",
      "         [[ 5.2803e-01,  7.1173e-01, -2.5735e-01,  1.5133e+00],\n",
      "          [ 4.1933e-01,  7.3343e-01,  5.7411e-02,  1.3845e-01],\n",
      "          [ 1.5512e-01,  2.8237e-03,  1.7845e-01,  6.8119e-02],\n",
      "          [ 3.9160e-01,  6.2254e-01, -5.8246e-02,  1.0559e-03]]]])\n",
      "Attention output = \n",
      "torch.Size([1, 4, 4, 4]) tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5254, 0.4746, 0.0000, 0.0000],\n",
      "          [0.0287, 0.7110, 0.2602, 0.0000],\n",
      "          [0.1126, 0.2572, 0.3485, 0.2818]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6313, 0.3687, 0.0000, 0.0000],\n",
      "          [0.2305, 0.1541, 0.6154, 0.0000],\n",
      "          [0.2410, 0.1546, 0.3406, 0.2637]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5422, 0.4578, 0.0000, 0.0000],\n",
      "          [0.1933, 0.2752, 0.5315, 0.0000],\n",
      "          [0.3103, 0.3001, 0.2051, 0.1845]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4437, 0.5563, 0.0000, 0.0000],\n",
      "          [0.0770, 0.2608, 0.6622, 0.0000],\n",
      "          [0.3026, 0.2914, 0.1472, 0.2588]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[-1.0260e+00,  9.6616e-01, -2.2913e+00,  1.8057e+00, -2.3983e-02,\n",
      "         -1.3353e+00, -1.6751e+00, -9.0489e-01,  1.5304e+00, -1.8514e+00,\n",
      "         -1.5147e+00,  9.2130e-01,  5.2803e-01,  7.1173e-01, -2.5735e-01,\n",
      "          1.5133e+00],\n",
      "        [-4.9481e-01,  3.6326e-02, -1.1817e+00,  1.2362e+00, -8.3502e-02,\n",
      "         -1.1530e+00, -1.0980e+00, -5.6330e-01,  1.3787e+00, -1.4910e+00,\n",
      "         -1.7919e+00,  7.9285e-01,  4.1933e-01,  7.3343e-01,  5.7411e-02,\n",
      "          1.3845e-01],\n",
      "        [ 3.3676e-01, -5.1201e-01,  8.9419e-02,  5.2970e-01,  1.4487e-01,\n",
      "         -1.7150e-01,  2.5771e-01, -4.2746e-01,  1.3513e+00, -8.6753e-01,\n",
      "         -1.4385e+00, -6.9414e-01,  1.5512e-01,  2.8237e-03,  1.7845e-01,\n",
      "          6.8119e-02],\n",
      "        [ 4.7802e-01, -4.6889e-02, -2.5458e-01,  4.7160e-01, -5.8512e-02,\n",
      "         -7.9001e-01, -3.0185e-01, -4.1176e-01,  1.2941e+00, -1.1401e+00,\n",
      "         -1.4758e+00,  2.7135e-01,  3.9160e-01,  6.2254e-01, -5.8246e-02,\n",
      "          1.0559e-03]])\n",
      "\n",
      "tensor([[[ 2.4178,  6.4858,  1.0955,  0.9115, -1.2320,  0.1593,  1.2339,\n",
      "           2.1548,  0.3546, -0.1995, -2.8314, -3.4712,  2.1251, -0.4063,\n",
      "           1.9201, -0.2223]],\n",
      "\n",
      "        [[-0.2797,  1.7911, -0.4420,  0.4950, -0.9662,  0.5428,  0.8577,\n",
      "           2.7110, -0.7737,  1.3644, -1.6425, -3.8073,  0.7040,  0.8110,\n",
      "          -0.2216,  1.3914]],\n",
      "\n",
      "        [[-1.1176,  0.8555,  0.8194, -0.6701, -0.0145, -0.0532,  1.6026,\n",
      "           1.1805,  1.7457,  0.0767,  0.3125,  1.6432,  0.1901,  1.7350,\n",
      "           1.1834,  2.1714]],\n",
      "\n",
      "        [[ 0.8713,  2.0605,  1.0126,  3.2653, -1.0411, -0.3180,  0.2010,\n",
      "           2.2730, -0.9352,  0.4167, -0.9338, -1.9150,  0.6066,  1.9513,\n",
      "          -0.8876,  0.3992]]])\n",
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[ 0.7908,  2.6168,  0.1973,  0.1147, -0.8474, -0.2229,  0.2594,\n",
      "           0.6727, -0.1353, -0.3840, -1.5653, -1.8525,  0.6594, -0.4768,\n",
      "           0.5674, -0.3942]],\n",
      "\n",
      "        [[-0.2944,  1.0971, -0.4035,  0.2262, -0.7558,  0.2582,  0.4699,\n",
      "           1.7153, -0.6264,  0.8104, -1.2103, -2.6650,  0.3666,  0.4385,\n",
      "          -0.2554,  0.8285]],\n",
      "\n",
      "        [[-2.0041,  0.1375,  0.0984, -1.5183, -0.8067, -0.8488,  0.9485,\n",
      "           0.4903,  1.1037, -0.7078, -0.4518,  0.9925, -0.5847,  1.0922,\n",
      "           0.4934,  1.5658]],\n",
      "\n",
      "        [[ 0.3119,  1.1703,  0.4139,  2.0400, -1.0685, -0.5466, -0.1719,\n",
      "           1.3237, -0.9920, -0.0162, -0.9911, -1.6993,  0.1209,  1.0915,\n",
      "          -0.9577, -0.0288]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "MEMORY =  tensor([[[ 0.0776, -0.8134, -0.6337,  0.2314, -0.1776,  0.3632, -1.3015,\n",
      "           1.5257, -0.5745,  1.5865,  0.4040,  0.2572, -1.0101,  1.9087,\n",
      "          -1.7131, -0.1305]],\n",
      "\n",
      "        [[-0.5666, -1.6901, -0.2921,  1.2146, -0.1103,  0.0079, -2.1157,\n",
      "           1.6536,  0.0881,  0.5683,  0.0386,  0.5650, -0.1463,  1.7027,\n",
      "          -0.6104, -0.3074]],\n",
      "\n",
      "        [[ 0.1136, -0.5709, -1.0399, -0.1699,  0.9766, -0.3185, -0.4165,\n",
      "           1.2750, -1.3753,  2.3096, -0.5499,  0.8579, -1.1126, -0.9835,\n",
      "          -0.0990,  1.1033]],\n",
      "\n",
      "        [[ 0.1810,  0.0224,  0.5764,  0.7853, -1.5473,  0.8328, -1.9498,\n",
      "           0.4452, -0.7445,  1.1691,  0.1031, -0.4145, -1.4597,  1.3111,\n",
      "          -0.5877,  1.2772]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([[[ 0.9514, -0.3307,  0.6432,  0.5748,  1.3066,  0.5686, -0.5934,\n",
      "           0.5253, -0.5276, -0.5529, -0.9860, -0.1614,  0.0147, -0.0634,\n",
      "          -0.4412,  0.7675,  0.0388, -0.6825, -0.9350,  0.4181, -0.5622,\n",
      "          -0.2204,  0.1494, -0.2058,  0.9466, -0.4971,  0.0339, -0.0201,\n",
      "          -0.0502, -1.3697, -0.3657, -0.2704]],\n",
      "\n",
      "        [[ 0.7483, -0.7711,  0.7212,  0.5145,  0.8625,  0.1507, -0.7507,\n",
      "           0.1990, -0.9502, -0.2645, -1.0429, -0.2423, -0.4215, -0.2730,\n",
      "           0.3532,  0.9262, -0.1735, -0.3143, -0.5732, -0.0503, -0.0970,\n",
      "          -0.3716, -0.2851, -0.8843,  0.6969, -0.8406, -0.0828, -0.0153,\n",
      "           0.4939, -0.9000,  0.4582, -0.3985]],\n",
      "\n",
      "        [[-0.8537, -0.7583,  0.1420, -1.1842, -0.0687,  1.0256, -0.4898,\n",
      "           1.0463,  0.2953, -0.4878, -0.2764,  0.8070,  0.7237,  0.0246,\n",
      "          -0.9059,  0.1544, -0.7933, -0.6954, -0.0235,  0.8652, -0.1659,\n",
      "           0.7528, -0.0084, -0.3710,  0.7004, -0.6308, -0.4044,  0.3146,\n",
      "          -0.4391,  0.3782, -0.7839,  0.5518]],\n",
      "\n",
      "        [[ 0.0912, -0.0339,  0.0884, -0.1742,  1.2265,  0.8607,  0.3285,\n",
      "           0.4813,  0.5597, -0.5625, -0.1868,  0.4669, -0.0212, -0.4559,\n",
      "          -0.7687,  0.2424, -0.5080, -1.1256, -0.8804,  0.4083, -0.6346,\n",
      "           0.3405, -0.6516,  0.2170,  0.8235, -0.7800, -0.0890, -0.1442,\n",
      "           0.8323, -1.3277, -0.4219, -0.9457]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Q_dec_0 = \n",
      "tensor([[[ 1.1264, -0.9937,  1.4197,  0.3692],\n",
      "         [ 0.8369, -0.3976,  1.7715, -0.2081],\n",
      "         [-0.8029, -0.4424, -0.4481, -0.0809],\n",
      "         [ 0.1404,  0.3561,  1.3061,  0.5648]],\n",
      "\n",
      "        [[ 0.3424,  0.6421,  1.0896, -0.3594],\n",
      "         [ 0.5932, -0.0213,  0.1751, -0.1013],\n",
      "         [ 0.2253, -0.3487, -1.3865,  0.5934],\n",
      "         [ 1.0678, -0.3641,  0.2272, -0.7018]],\n",
      "\n",
      "        [[ 0.2477,  0.7129, -1.5834, -0.2458],\n",
      "         [ 0.7496, -0.1440, -0.8612,  0.5405],\n",
      "         [ 1.2828, -0.9602, -0.9851, -0.0296],\n",
      "         [ 0.6308,  0.1396,  0.1332,  0.7768]],\n",
      "\n",
      "        [[-0.5755,  0.7306, -0.5634, -1.4579],\n",
      "         [-1.0340,  0.3903, -0.6624, -0.7871],\n",
      "         [ 0.8031, -0.9828,  0.0296,  0.1481],\n",
      "         [-1.5224,  1.3332, -0.5388, -0.9296]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 0.9514, -0.3307,  0.6432,  0.5748],\n",
      "         [ 0.7483, -0.7711,  0.7212,  0.5145],\n",
      "         [-0.8537, -0.7583,  0.1420, -1.1842],\n",
      "         [ 0.0912, -0.0339,  0.0884, -0.1742]],\n",
      "\n",
      "        [[ 1.3066,  0.5686, -0.5934,  0.5253],\n",
      "         [ 0.8625,  0.1507, -0.7507,  0.1990],\n",
      "         [-0.0687,  1.0256, -0.4898,  1.0463],\n",
      "         [ 1.2265,  0.8607,  0.3285,  0.4813]],\n",
      "\n",
      "        [[-0.5276, -0.5529, -0.9860, -0.1614],\n",
      "         [-0.9502, -0.2645, -1.0429, -0.2423],\n",
      "         [ 0.2953, -0.4878, -0.2764,  0.8070],\n",
      "         [ 0.5597, -0.5625, -0.1868,  0.4669]],\n",
      "\n",
      "        [[ 0.0147, -0.0634, -0.4412,  0.7675],\n",
      "         [-0.4215, -0.2730,  0.3532,  0.9262],\n",
      "         [ 0.7237,  0.0246, -0.9059,  0.1544],\n",
      "         [-0.0212, -0.4559, -0.7687,  0.2424]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.0388, -0.6825, -0.9350,  0.4181],\n",
      "         [-0.1735, -0.3143, -0.5732, -0.0503],\n",
      "         [-0.7933, -0.6954, -0.0235,  0.8652],\n",
      "         [-0.5080, -1.1256, -0.8804,  0.4083]],\n",
      "\n",
      "        [[-0.5622, -0.2204,  0.1494, -0.2058],\n",
      "         [-0.0970, -0.3716, -0.2851, -0.8843],\n",
      "         [-0.1659,  0.7528, -0.0084, -0.3710],\n",
      "         [-0.6346,  0.3405, -0.6516,  0.2170]],\n",
      "\n",
      "        [[ 0.9466, -0.4971,  0.0339, -0.0201],\n",
      "         [ 0.6969, -0.8406, -0.0828, -0.0153],\n",
      "         [ 0.7004, -0.6308, -0.4044,  0.3146],\n",
      "         [ 0.8235, -0.7800, -0.0890, -0.1442]],\n",
      "\n",
      "        [[-0.0502, -1.3697, -0.3657, -0.2704],\n",
      "         [ 0.4939, -0.9000,  0.4582, -0.3985],\n",
      "         [-0.4391,  0.3782, -0.7839,  0.5518],\n",
      "         [ 0.8323, -1.3277, -0.4219, -0.9457]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attn weight =  tensor([[[[0.3705, 0.4299, 0.0839, 0.1157],\n",
      "          [0.3436, 0.3715, 0.1354, 0.1495],\n",
      "          [0.1551, 0.1827, 0.4227, 0.2394],\n",
      "          [0.3496, 0.3297, 0.1252, 0.1955]],\n",
      "\n",
      "         [[0.2235, 0.1763, 0.1970, 0.4032],\n",
      "          [0.2785, 0.2459, 0.1811, 0.2945],\n",
      "          [0.2954, 0.3059, 0.2538, 0.1449],\n",
      "          [0.2999, 0.2812, 0.1116, 0.3073]],\n",
      "\n",
      "         [[0.3081, 0.3424, 0.1768, 0.1727],\n",
      "          [0.2274, 0.1906, 0.2948, 0.2872],\n",
      "          [0.2396, 0.1638, 0.2736, 0.3231],\n",
      "          [0.1740, 0.1500, 0.3461, 0.3299]],\n",
      "\n",
      "         [[0.2148, 0.1606, 0.3223, 0.3023],\n",
      "          [0.2376, 0.2064, 0.2487, 0.3073],\n",
      "          [0.2323, 0.2213, 0.2807, 0.2658],\n",
      "          [0.2536, 0.2305, 0.2362, 0.2797]]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "tensor([[[[0.3705, 0.4299, 0.0839, 0.1157],\n",
      "          [0.3436, 0.3715, 0.1354, 0.1495],\n",
      "          [0.1551, 0.1827, 0.4227, 0.2394],\n",
      "          [0.3496, 0.3297, 0.1252, 0.1955]],\n",
      "\n",
      "         [[0.2235, 0.1763, 0.1970, 0.4032],\n",
      "          [0.2785, 0.2459, 0.1811, 0.2945],\n",
      "          [0.2954, 0.3059, 0.2538, 0.1449],\n",
      "          [0.2999, 0.2812, 0.1116, 0.3073]],\n",
      "\n",
      "         [[0.3081, 0.3424, 0.1768, 0.1727],\n",
      "          [0.2274, 0.1906, 0.2948, 0.2872],\n",
      "          [0.2396, 0.1638, 0.2736, 0.3231],\n",
      "          [0.1740, 0.1500, 0.3461, 0.3299]],\n",
      "\n",
      "         [[0.2148, 0.1606, 0.3223, 0.3023],\n",
      "          [0.2376, 0.2064, 0.2487, 0.3073],\n",
      "          [0.2323, 0.2213, 0.2807, 0.2658],\n",
      "          [0.2536, 0.2305, 0.2362, 0.2797]]]], grad_fn=<SoftmaxBackward0>)\n",
      "ATT =  tensor([[[[-0.1855, -0.5766, -0.6967,  0.2531],\n",
      "          [-0.2344, -0.6137, -0.6690,  0.3032],\n",
      "          [-0.4827, -0.7268, -0.4705,  0.5191],\n",
      "          [-0.2422, -0.6493, -0.6909,  0.3177]],\n",
      "\n",
      "         [[-0.4313,  0.1709, -0.2813, -0.1875],\n",
      "          [-0.3974,  0.0839, -0.2219, -0.2780],\n",
      "          [-0.3298,  0.0616, -0.1397, -0.3940],\n",
      "          [-0.4094,  0.0180, -0.2365, -0.2851]],\n",
      "\n",
      "         [[ 0.7963, -0.6872, -0.1048,  0.0193],\n",
      "          [ 0.7911, -0.6832, -0.1529,  0.0438],\n",
      "          [ 0.7986, -0.6813, -0.1448,  0.0321],\n",
      "          [ 0.7833, -0.6882, -0.1758,  0.0555]],\n",
      "\n",
      "         [[ 0.1786, -0.7182, -0.3852, -0.2302],\n",
      "          [ 0.2365, -0.8251, -0.3170, -0.2998],\n",
      "          [ 0.1956, -0.7640, -0.3157, -0.2475],\n",
      "          [ 0.2302, -0.8368, -0.2903, -0.2946]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Attention output = \n",
      "torch.Size([1, 4, 4, 4]) tensor([[[[0.3705, 0.4299, 0.0839, 0.1157],\n",
      "          [0.3436, 0.3715, 0.1354, 0.1495],\n",
      "          [0.1551, 0.1827, 0.4227, 0.2394],\n",
      "          [0.3496, 0.3297, 0.1252, 0.1955]],\n",
      "\n",
      "         [[0.2235, 0.1763, 0.1970, 0.4032],\n",
      "          [0.2785, 0.2459, 0.1811, 0.2945],\n",
      "          [0.2954, 0.3059, 0.2538, 0.1449],\n",
      "          [0.2999, 0.2812, 0.1116, 0.3073]],\n",
      "\n",
      "         [[0.3081, 0.3424, 0.1768, 0.1727],\n",
      "          [0.2274, 0.1906, 0.2948, 0.2872],\n",
      "          [0.2396, 0.1638, 0.2736, 0.3231],\n",
      "          [0.1740, 0.1500, 0.3461, 0.3299]],\n",
      "\n",
      "         [[0.2148, 0.1606, 0.3223, 0.3023],\n",
      "          [0.2376, 0.2064, 0.2487, 0.3073],\n",
      "          [0.2323, 0.2213, 0.2807, 0.2658],\n",
      "          [0.2536, 0.2305, 0.2362, 0.2797]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_0\n",
      "tensor([[-0.1855, -0.5766, -0.6967,  0.2531, -0.4313,  0.1709, -0.2813, -0.1875,\n",
      "          0.7963, -0.6872, -0.1048,  0.0193,  0.1786, -0.7182, -0.3852, -0.2302],\n",
      "        [-0.2344, -0.6137, -0.6690,  0.3032, -0.3974,  0.0839, -0.2219, -0.2780,\n",
      "          0.7911, -0.6832, -0.1529,  0.0438,  0.2365, -0.8251, -0.3170, -0.2998],\n",
      "        [-0.4827, -0.7268, -0.4705,  0.5191, -0.3298,  0.0616, -0.1397, -0.3940,\n",
      "          0.7986, -0.6813, -0.1448,  0.0321,  0.1956, -0.7640, -0.3157, -0.2475],\n",
      "        [-0.2422, -0.6493, -0.6909,  0.3177, -0.4094,  0.0180, -0.2365, -0.2851,\n",
      "          0.7833, -0.6882, -0.1758,  0.0555,  0.2302, -0.8368, -0.2903, -0.2946]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "tensor([[[-1.8688e-01,  2.8021e+00, -3.8453e-02,  2.8037e-03, -8.4053e-01,\n",
      "          -8.1155e-01,  3.7839e-01,  6.2611e-01, -2.5406e-01,  3.1409e-01,\n",
      "          -1.7497e+00, -1.2129e+00,  4.4530e-01, -1.2212e+00,  1.0988e+00,\n",
      "          -1.1224e+00]],\n",
      "\n",
      "        [[-1.3363e+00,  1.2765e+00, -5.7569e-01,  2.4573e-02, -8.1529e-01,\n",
      "          -3.5427e-01,  6.0473e-01,  1.6942e+00, -7.7991e-01,  1.5508e+00,\n",
      "          -1.4916e+00, -1.9056e+00,  1.0317e-01, -2.7979e-01,  2.8269e-01,\n",
      "           1.2390e-02]],\n",
      "\n",
      "        [[-3.0902e+00,  2.9164e-01,  6.6775e-02, -1.6819e+00, -9.3574e-01,\n",
      "          -1.4855e+00,  1.1888e+00,  7.0985e-01,  9.7018e-01,  1.2751e-01,\n",
      "          -7.0065e-01,  1.9179e+00, -1.0057e+00,  3.3671e-01,  1.0325e+00,\n",
      "           7.5059e-01]],\n",
      "\n",
      "        [[-7.6776e-01,  1.3630e+00,  2.3890e-01,  1.7718e+00, -1.1720e+00,\n",
      "          -1.1804e+00, -1.8271e-02,  1.2930e+00, -1.1703e+00,  7.3390e-01,\n",
      "          -1.2910e+00, -9.2321e-01, -1.8581e-01,  3.9111e-01, -3.7845e-01,\n",
      "          -8.5944e-01]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.0763,  2.9128,  0.0722,  0.1134, -0.7299, -0.7009,  0.4890,\n",
      "           0.7367, -0.1434,  0.4247, -1.6391, -1.1023,  0.5559, -1.1106,\n",
      "           1.2095, -1.0118]],\n",
      "\n",
      "        [[-1.2120,  1.4008, -0.4514,  0.1489, -0.6910, -0.2299,  0.7291,\n",
      "           1.8186, -0.6556,  1.6751, -1.3672, -1.7812,  0.2275, -0.1555,\n",
      "           0.4070,  0.1367]],\n",
      "\n",
      "        [[-2.9960,  0.3858,  0.1610, -1.5877, -0.8416, -1.3913,  1.2830,\n",
      "           0.8040,  1.0644,  0.2217, -0.6065,  2.0121, -0.9115,  0.4309,\n",
      "           1.1267,  0.8448]],\n",
      "\n",
      "        [[-0.6331,  1.4977,  0.3736,  1.9065, -1.0373, -1.0457,  0.1164,\n",
      "           1.4277, -1.0356,  0.8686, -1.1563, -0.7885, -0.0511,  0.5258,\n",
      "          -0.2438, -0.7248]]], grad_fn=<SubBackward0>)\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-0.0710,  2.7128,  0.0672,  0.1056, -0.6798, -0.6528,  0.4554,\n",
      "           0.6862, -0.1336,  0.3956, -1.5266, -1.0266,  0.5178, -1.0343,\n",
      "           1.1264, -0.9423]],\n",
      "\n",
      "        [[-1.1912,  1.3768, -0.4436,  0.1463, -0.6791, -0.2260,  0.7165,\n",
      "           1.7873, -0.6443,  1.6463, -1.3437, -1.7506,  0.2236, -0.1528,\n",
      "           0.4000,  0.1344]],\n",
      "\n",
      "        [[-2.3869,  0.3074,  0.1282, -1.2650, -0.6705, -1.1085,  1.0222,\n",
      "           0.6406,  0.8480,  0.1766, -0.4832,  1.6031, -0.7262,  0.3433,\n",
      "           0.8977,  0.6731]],\n",
      "\n",
      "        [[-0.6476,  1.5321,  0.3822,  1.9503, -1.0611, -1.0698,  0.1191,\n",
      "           1.4605, -1.0594,  0.8886, -1.1829, -0.8066, -0.0523,  0.5379,\n",
      "          -0.2494, -0.7414]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "mean =  tensor([[[-0.1670]],\n",
      "\n",
      "        [[-0.2212]],\n",
      "\n",
      "        [[-0.0720]],\n",
      "\n",
      "        [[-0.2758]]], grad_fn=<MeanBackward1>)\n",
      "std =  tensor([[[1.2243]],\n",
      "\n",
      "        [[1.0995]],\n",
      "\n",
      "        [[0.9733]],\n",
      "\n",
      "        [[1.1266]]], grad_fn=<StdBackward0>)\n",
      "tensor([[[-0.6558,  2.3651, -0.6266, -0.0843, -0.4387,  0.0078,  0.9118,\n",
      "           0.4723,  0.4488,  0.2983, -1.8572, -1.4654,  1.5229, -1.2328,\n",
      "           1.9739, -1.6398]],\n",
      "\n",
      "        [[-1.0450,  0.8596, -0.9921, -0.1795, -0.3950,  0.1864,  0.9412,\n",
      "           1.8923, -0.6174,  1.4856, -1.4721, -2.3637,  0.6398,  0.5323,\n",
      "           0.8395, -0.3120]],\n",
      "\n",
      "        [[-1.6807,  0.1243, -0.1994, -1.8914,  0.5709, -1.0652,  0.8269,\n",
      "           0.5513,  0.3123, -0.5426, -1.0425,  0.7502, -0.0781,  1.0200,\n",
      "           1.6288,  0.7152]],\n",
      "\n",
      "        [[-0.2033,  1.2900, -0.1580,  2.0363, -0.7522, -0.6666,  0.1353,\n",
      "           1.4065, -1.3857,  0.7341, -1.6700, -1.2598,  0.0709,  1.7300,\n",
      "          -0.0473, -1.2601]]], grad_fn=<SubBackward0>)\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-0.5357,  1.9318, -0.5118, -0.0689, -0.3583,  0.0063,  0.7447,\n",
      "           0.3858,  0.3666,  0.2436, -1.5170, -1.1969,  1.2439, -1.0070,\n",
      "           1.6123, -1.3394]],\n",
      "\n",
      "        [[-0.9504,  0.7818, -0.9023, -0.1632, -0.3592,  0.1695,  0.8561,\n",
      "           1.7210, -0.5616,  1.3512, -1.3389, -2.1498,  0.5819,  0.4841,\n",
      "           0.7636, -0.2838]],\n",
      "\n",
      "        [[-1.7267,  0.1277, -0.2049, -1.9432,  0.5866, -1.0944,  0.8495,\n",
      "           0.5664,  0.3209, -0.5575, -1.0710,  0.7707, -0.0802,  1.0479,\n",
      "           1.6734,  0.7348]],\n",
      "\n",
      "        [[-0.1805,  1.1451, -0.1402,  1.8075, -0.6677, -0.5917,  0.1201,\n",
      "           1.2485, -1.2300,  0.6516, -1.4824, -1.1182,  0.0630,  1.5356,\n",
      "          -0.0420, -1.1185]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([4, 1, 16]) torch.Size([4, 1, 16])\n",
      "### Decoder Done ###\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "final_op = get_all_intermediate_outputs_mask(src_data, tgt_data[:-1, :], state_dict = state_dict1, num_heads = num_heads, num_encoder_layers = num_encoder_layers , num_decoder_layers = num_encoder_layers, d_model=d_model,  d_ff = d_ff, tgt_mask = tgt_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3775,  0.0734, -0.8096,  0.1469, -0.4905,  0.3739, -0.2705,\n",
       "          -0.9242,  0.6274, -0.6122,  0.2593, -0.1688,  0.1323,  1.3099,\n",
       "           0.7643, -0.6106, -0.1371,  0.7552, -0.5415,  0.7422]],\n",
       "\n",
       "        [[ 0.4344,  0.3561, -0.4982, -0.4326, -0.2377,  0.8758, -1.0641,\n",
       "          -0.3423,  0.3772, -0.4452,  0.6519, -0.2666, -0.0646,  0.7807,\n",
       "           0.5272, -0.6573,  0.0976,  1.0804, -0.9153,  1.1743]],\n",
       "\n",
       "        [[-0.5873,  0.2161, -0.6100, -0.0774, -0.5958,  0.6877, -0.2677,\n",
       "          -0.4463,  0.5172,  1.1137,  0.1040, -0.3007, -1.1872, -0.2548,\n",
       "           0.6388,  0.0914,  0.0290, -0.0519,  0.3834, -0.1068]],\n",
       "\n",
       "        [[ 0.7888,  0.7641, -0.1245, -0.0249,  0.1865, -0.0143, -0.9726,\n",
       "          -0.9177,  0.3889, -0.3268,  0.1702, -0.8667,  0.2136,  0.3825,\n",
       "           0.2635, -0.9925, -0.4424,  1.0395, -1.2948,  0.7781]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict[\"transformer.encoder.layers.0.self_attn.out_proj.weight\"]\n",
    "\n",
    "final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['src_embedding.weight', 'tgt_embedding.weight', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.norm.weight', 'transformer.encoder.norm.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
